{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sat-A/s5-jax/blob/main/s5-Predictor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# S5-JAX Predictor"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "1YMG3-5Hg6s_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The goal of this exercise is to adapt an annotated implementation of S5 https://github.com/JPGoodale/annotated-s5 that was made for classification tasks to regression tasks."
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "dXRZLfGeg6tA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Runtime Setup\n",
        "Ensure runtime is set to GPU to ensure gpu parallelisation speedup"
      ],
      "metadata": {
        "id": "WIpj_s_ZcaAJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dm-haiku\n",
        "!pip install hippox"
      ],
      "metadata": {
        "id": "XylzIGnbooEy",
        "outputId": "22f9aaa3-07b0-4f4e-cb5c-f98d6ca8841b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: dm-haiku in /usr/local/lib/python3.12/dist-packages (0.0.15)\n",
            "Requirement already satisfied: absl-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from dm-haiku) (1.4.0)\n",
            "Requirement already satisfied: jmp>=0.0.2 in /usr/local/lib/python3.12/dist-packages (from dm-haiku) (0.0.4)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.12/dist-packages (from dm-haiku) (2.0.2)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.12/dist-packages (from dm-haiku) (0.9.0)\n",
            "Requirement already satisfied: hippox in /usr/local/lib/python3.12/dist-packages (0.0.9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "outputs": [],
      "source": [
        "# Let's go ahead and import hippox and the core JAX libraries we will be using:\n",
        "import jax\n",
        "import numpy as np\n",
        "import jax.numpy as jnp\n",
        "import haiku as hk\n",
        "from hippox.main import Hippo\n",
        "from typing import Optional\n",
        "# New imports for stock data\n",
        "import yfinance as yf\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch  # Make sure torch is imported if not already\n",
        "from torch.utils.data import DataLoader, Dataset as TorchDataset\n",
        "from typing import NamedTuple\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n",
          "is_executing": true
        },
        "id": "YYWAK_LIg6tB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First we'll define some helper functions for discretization and timescale initialization as the SSM equation is naturally continuous and must be made discrete to be unrolled as a linear recurrence like standard RNNs."
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "psmzqs8Xg6tB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "outputs": [],
      "source": [
        "# Here we are just using the zero order hold method for its sheer simplicity, with A, B and delta_t denoting the\n",
        "# state matrix, input matrix and change in timescale respectively.\n",
        "\n",
        "def discretize(A, B, delta_t):\n",
        "    Identity = jnp.ones(A.shape[0])\n",
        "    _A = jnp.exp(A*delta_t)\n",
        "    _B = (1/A * (_A-Identity)) * B\n",
        "    return _A, _B\n",
        "\n",
        "# This is a function used to initialize the trainable timescale parameter.\n",
        "def log_step_initializer(dt_min=0.001, dt_max=0.1):\n",
        "    def init(shape, dtype):\n",
        "        uniform = hk.initializers.RandomUniform()\n",
        "        return uniform(shape, dtype)*(jnp.log(dt_max) - jnp.log(dt_min)) + jnp.log(dt_min)\n",
        "    return init\n",
        "\n",
        "# Taken directly from https://github.com/deepmind/dm-haiku/blob/main/haiku/_src/recurrent.py\n",
        "def add_batch(nest, batch_size: Optional[int]):\n",
        "    broadcast = lambda x: jnp.broadcast_to(x, (batch_size,) + x.shape)\n",
        "    return jax.tree_util.tree_map(broadcast, nest)"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n",
          "is_executing": true
        },
        "id": "Ous4HGubg6tB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The linear SSM equation is as follows:\n",
        "$$ x_0(t) = Ax(t) + Bu(t) $$\n",
        "$$ y(t) = Cx(t) + Du(t) $$\n",
        "\n",
        " We will now implement it as a recurrent Haiku module:"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "SaJ9vnolg6tB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "outputs": [],
      "source": [
        "class LinearSSM(hk.RNNCore):\n",
        "    def __init__(self, state_size: int, name: Optional[str] = None):\n",
        "        super(LinearSSM, self).__init__(name=name)\n",
        "        # We won't get into the basis measure families here, just note that they are basically just the\n",
        "        # type of orthogonal polynomial we initialize with, the scaled Legendre measure (LegS) introduced\n",
        "        # in the original HiPPO paper is pretty much the standard initialization and is what is used in the\n",
        "        # main experiments in the S5 paper. I will also note that the Hippo class uses the diagonal representation\n",
        "        # of the state matrix by default, as this has become the standard in neural SSMs since shown to be\n",
        "        # equally effective as the diagonal plus low rank representation in https://arxiv.org/abs/2203.14343\n",
        "        # and then formalized in https://arxiv.org/abs/2206.11893.\n",
        "\n",
        "        _hippo = Hippo(state_size=state_size, basis_measure='legs')\n",
        "        # Must be called for parameters to be initialized\n",
        "        _hippo()\n",
        "\n",
        "        # We register the real and imaginary components of the state matrix A as separate parameters because\n",
        "        # they will have separate gradients in training, they will be conjoined back together and then discretized\n",
        "        # but this will simply be backpropagated through as a transformation of the lambda real and imaginary\n",
        "        # parameters (lambda is just what we call the diagonalized state matrix).\n",
        "\n",
        "        self._lambda_real = hk.get_parameter(\n",
        "            'lambda_real',\n",
        "            shape=[state_size,],\n",
        "            init=_hippo.lambda_initializer('real')\n",
        "        )\n",
        "        self._lambda_imag = hk.get_parameter(\n",
        "            'lambda_imaginary',\n",
        "            shape=[state_size,],\n",
        "            init=_hippo.lambda_initializer('imaginary')\n",
        "        )\n",
        "        self._A = self._lambda_real + 1j * self._lambda_imag\n",
        "\n",
        "       # For now, these initializations of the input and output matrices B and C match the S4D\n",
        "        # parameterization for demonstration purposes, we will implement the S5 versions later.\n",
        "\n",
        "        self._B = hk.get_parameter(\n",
        "            'B',\n",
        "            shape=[state_size,],\n",
        "            init=_hippo.b_initializer()\n",
        "        )\n",
        "        self._C = hk.get_parameter(\n",
        "            'C',\n",
        "            shape=[state_size, 2],\n",
        "            init=hk.initializers.RandomNormal(stddev=0.5**0.5)\n",
        "        )\n",
        "        self._output_matrix = self._C[..., 0] + 1j * self._C[..., 1]\n",
        "\n",
        "        # This feed-through matrix basically acts as a residual connection.\n",
        "        self._D = hk.get_parameter(\n",
        "            'D',\n",
        "            [1,],\n",
        "            init=jnp.ones,\n",
        "        )\n",
        "\n",
        "        self._delta_t = hk.get_parameter(\n",
        "            'delta_t',\n",
        "            shape=[1,],\n",
        "            init=log_step_initializer()\n",
        "        )\n",
        "        timescale = jnp.exp(self._delta_t)\n",
        "\n",
        "        self._state_matrix, self._input_matrix = discretize(self._A, self._B, timescale)\n",
        "\n",
        "    def __call__(self, inputs, prev_state):\n",
        "        u = inputs[:, jnp.newaxis]\n",
        "        new_state = self._state_matrix @ prev_state + self._input_matrix @ u\n",
        "        y_s = self._output_matrix @ new_state\n",
        "        out = y_s.reshape(-1).real + self._D * u\n",
        "        return out, new_state\n",
        "\n",
        "    def initial_state(self, batch_size: Optional[int] = None):\n",
        "        state = jnp.zeros([self._state_size])\n",
        "        if batch_size is not None:\n",
        "            state = add_batch(state, batch_size)\n",
        "        return state"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n",
          "is_executing": true
        },
        "id": "PySpa6EMg6tC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You may notice that this looks an awful lot like a vanilla RNN cell, just with our special parameterization and without any activations, hence being a linear recurrence. I have initialized it as an instance of Haiku's RNN.Core abstract base class so that it can be unrolled using either the hk.dynamic_unroll or hk.static_unroll functions like any other recurrent module, however, if you are familiar with any of the S4 models you may be noticing that there's something crucial missing here: the convolutional representation. One of the key contributions of the S4 paper was its demonstration that the SSM ODE can be represented as either a linear recurrence, as above, for efficient inference, or as a global convolution for much faster training. That paper and the following papers then go on to present various kernels for efficiently computing this convolution with Fast Fourier Transforms, highly improving the computational efficiency of the model. Then why have we omitted them? Because the S5 architecture which we are about to explore simplifies all this by providing a purely recurrent representation in both training and inference, it does this by using a parallel recurrence that actually looks alot like a convolution itself! From the paper:\n",
        "\n",
        "    \"We use parallel scans to efficiently compute the states of a discretized linear SSM. Given a binary associative operator • (i.e. (a • b) • c = a • (b • c)) and a sequence of L elements [a1, a2, ..., aL], the scan operation (sometimes referred to as all-prefix-sum) returns the sequence [a1, (a1 • a2), ..., (a1 • a2 • ... • aL)].\"\n",
        "\n",
        "Let's see what this looks like in code, taken straight from the original author's implementation:"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "S7kVHuCkg6tC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "outputs": [],
      "source": [
        "@jax.vmap\n",
        "def binary_operator(q_i, q_j):\n",
        "    A_i, b_i = q_i\n",
        "    A_j, b_j = q_j\n",
        "    return A_j * A_i, A_j * b_i + b_j\n",
        "\n",
        "def parallel_scan(A, B, C, inputs):\n",
        "    A_elements = A * jnp.ones((inputs.shape[0], A.shape[0]))\n",
        "    Bu_elements = jax.vmap(lambda u: B @ u)(inputs)\n",
        "    # Jax's built-in associative scan really comes in handy here as it executes a similar scan\n",
        "    # operation as used in a normal recurrent unroll but is specifically tailored to fit an associative\n",
        "    # operation like the one described in the paper.\n",
        "    _, xs = jax.lax.associative_scan(binary_operator, (A_elements, Bu_elements))\n",
        "    return jax.vmap(lambda x: (C @ x).real)(xs)"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n",
          "is_executing": true
        },
        "id": "1gxWu4Yrg6tC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's that simple! In the original S4 we would have had to apply an independent singe-input, single-output (SISO) SSM for each feature of the input sequence such as in this excerpt from Sasha Rush's Flax implementation:"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "3KfEw2x7g6tC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```python\n",
        "def cloneLayer(layer):\n",
        "    return flax.linen.vmap(\n",
        "        layer,\n",
        "        in_axes=1,\n",
        "        out_axes=1,\n",
        "        variable_axes={\"params\": 1, \"cache\": 1, \"prime\": 1},\n",
        "        split_rngs={\"params\": True},\n",
        "    )\n",
        "SSMLayer = cloneLayer(SSMLayer)```\n",
        "\n"
      ],
      "metadata": {
        "id": "gqDe8Dxuc33-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Whereas in the S5 we process the entire sequence in one multi-input, multi-output (MIMO) layer."
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "fvQsAYVtg6tD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's now rewrite our Module as a full S5 layer using this new method, we will be adding a few extra conditional arguments as well as changing some parameterization to match the original paper, but we'll walk through the reason for all these changes below."
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "vC7ht3zAg6tD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "outputs": [],
      "source": [
        "# First we add a new helper function for the timescale initialization, this one just takes the previous\n",
        "# log_step_initializer and stores a bunch of them in an array since our model is now multi-in, multi-out.\n",
        "\n",
        "def init_log_steps(shape, dtype):\n",
        "    H = shape[0]\n",
        "    log_steps = []\n",
        "    for i in range(H):\n",
        "        log_step = log_step_initializer()(shape=(1,), dtype=dtype)\n",
        "        log_steps.append(log_step)\n",
        "\n",
        "    return jnp.array(log_steps)"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n",
          "is_executing": true
        },
        "id": "fmsha-9Fg6tD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "outputs": [],
      "source": [
        "# We will also rewrite our discretization for the MIMO context\n",
        "def discretize(A, B, delta_t):\n",
        "    Identity = jnp.ones(A.shape[0])\n",
        "    _A = jnp.exp(A*delta_t)\n",
        "    _B = (1/A * (_A-Identity))[..., None] * B\n",
        "    return _A, _B"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n",
          "is_executing": true
        },
        "id": "J6NX3QPCg6tD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "outputs": [],
      "source": [
        "class S5(hk.Module):\n",
        "    def __init__(self,\n",
        "                 state_size: int,\n",
        "\n",
        "                 # Now that we're MIMO we'll need to know the number of input features, commonly\n",
        "                 # referred to as the dimension of the model.\n",
        "                 d_model: int,\n",
        "\n",
        "                 # We must also now specify the number of blocks that we will split our matrices\n",
        "                 # into due to the MIMO context.\n",
        "                 n_blocks: int,\n",
        "\n",
        "                 # Short for conjugate symmetry, because our state matrix is complex we can half\n",
        "                 # the size of it since complex numbers are a real and imaginary number joined together,\n",
        "                 # this is not new to the S5, we just didn't mention it above.\n",
        "                 conj_sym: bool = True,\n",
        "\n",
        "                 # Another standard SSM argument that we omitted above for simplicity's sake,\n",
        "                 # this forces the real part of the state matrix to be negative for better\n",
        "                 # stability, especially in autoregressive tasks.\n",
        "                 clip_eigns: bool = False,\n",
        "\n",
        "                 # Like most RNNs, the S5 can be run in both directions if need be.\n",
        "                 bidirectional: bool = False,\n",
        "\n",
        "                 # Rescales delta_t for varying input resolutions, such as different audio\n",
        "                 # sampling rates.\n",
        "                 step_rescale: float = 1.0,\n",
        "                 name: Optional[str] = None\n",
        "    ):\n",
        "        super(S5, self).__init__(name=name)\n",
        "        self.conj_sym = conj_sym\n",
        "        self.bidirectional = bidirectional\n",
        "\n",
        "        # Note that the Hippo class takes conj_sym as an argument and will automatically half\n",
        "        # the state size provided in its initialization, which is why we need to provide a local\n",
        "        # state size that matches this for the shape argument in hk.get_parameter().\n",
        "\n",
        "        if conj_sym:\n",
        "            _state_size = state_size // 2\n",
        "        else:\n",
        "            _state_size = state_size\n",
        "\n",
        "        # With block_diagonal set as True and the number of blocks provided, our Hippo class\n",
        "        # will automatically handle this change of structure.\n",
        "\n",
        "        _hippo = Hippo(\n",
        "            state_size=state_size,\n",
        "            basis_measure='legs',\n",
        "            conj_sym=conj_sym,\n",
        "            block_diagonal=True,\n",
        "            n_blocks=n_blocks,\n",
        "        )\n",
        "        _hippo()\n",
        "\n",
        "        self._lambda_real = hk.get_parameter(\n",
        "            'lambda_real',\n",
        "            [_state_size],\n",
        "            init=_hippo.lambda_initializer('real')\n",
        "        )\n",
        "        self._lambda_imag = hk.get_parameter(\n",
        "            'lambda_imaginary',\n",
        "            [_state_size],\n",
        "            init=_hippo.lambda_initializer('imaginary')\n",
        "        )\n",
        "        if clip_eigns:\n",
        "            self._lambda = jnp.clip(self._lambda_real, None, -1e-4) + 1j * self._lambda_imag\n",
        "        else:\n",
        "            self._A = self._lambda_real + 1j * self._lambda_imag\n",
        "\n",
        "        # If you recall, I mentioned above that we are automatically using a diagonalized version of\n",
        "        # the HiPPO state matrix rather than the pure one, due to it being very hard to efficiently\n",
        "        # compute. I will now go into a little more detail on how this diagonal representation is\n",
        "        # derived, as it is important for how we initialize the input and output matrices. The diagonal\n",
        "        # decomposition of our state matrix is based on equivalence relation on the SSM parameters:\n",
        "        # (A, B, C) ∼ (V−1AV ,V−1B, CV) with V being the eigenvector of our original A matrix and V-1\n",
        "        # being the inverse eigenvector. The Hippo class has already performed the decomposition of A\n",
        "        # into (V-1AV) automatically, but we have not yet performed the decomposition of B and C, we will\n",
        "        # use the eigenvector_transform class method for that below, but first we must initialize B and C\n",
        "        # as normal distributions, lecun normal and truncated normal respectively. I will note that there\n",
        "        # are a few other options provided for C in the original repository but, to keep it simple, we will\n",
        "        # just use one here.\n",
        "\n",
        "        b_init = hk.initializers.VarianceScaling()\n",
        "        b_shape = [state_size, d_model]\n",
        "        b_init = b_init(b_shape, dtype=jnp.complex64)\n",
        "        self._B = hk.get_parameter(\n",
        "            'B',\n",
        "            [_state_size, d_model, 2],\n",
        "            init=_hippo.eigenvector_transform(b_init,  concatenate=True),\n",
        "        )\n",
        "        B = self._B[..., 0] + 1j * self._B[..., 1]\n",
        "\n",
        "        c_init = hk.initializers.TruncatedNormal()\n",
        "        c_shape = [d_model, state_size, 2]\n",
        "        c_init = c_init(c_shape, dtype=jnp.complex64)\n",
        "        self._C = hk.get_parameter(\n",
        "            'C',\n",
        "            [d_model, _state_size, 2],\n",
        "            init=_hippo.eigenvector_transform(c_init, inverse=False, concatenate=True),\n",
        "        )\n",
        "        # We need two output heads if bidirectional is True.\n",
        "        if bidirectional:\n",
        "            self._C2 = hk.get_parameter(\n",
        "                'C2',\n",
        "                [d_model, _state_size, 2],\n",
        "                init=_hippo.eigenvector_transform(c_init, inverse=False, concatenate=True),\n",
        "            )\n",
        "            C1 = self._C[..., 0] + 1j * self._C[..., 1]\n",
        "            C2 = self._C2[..., 0] + 1j * self._C2[..., 1]\n",
        "            self._output_matrix = jnp.concatenate((C1, C2), axis=-1)\n",
        "        else:\n",
        "            self._output_matrix = self._C[..., 0] + 1j * self._C[..., 1]\n",
        "\n",
        "        self._D = hk.get_parameter(\n",
        "            'D',\n",
        "            [d_model,],\n",
        "            init=hk.initializers.RandomNormal(stddev=1.0)\n",
        "        )\n",
        "\n",
        "        self._delta_t = hk.get_parameter(\n",
        "            'delta_T',\n",
        "            [_state_size, 1],\n",
        "            init=init_log_steps\n",
        "        )\n",
        "        timescale = step_rescale * jnp.exp(self._delta_t[:, 0])\n",
        "\n",
        "        # We could also use the bilinear discretization method, but we'll just stick to zoh for now.\n",
        "        self._state_matrix, self._input_matrix = discretize(self._A, B, timescale)\n",
        "\n",
        "\n",
        "    def __call__(self, inputs):\n",
        "        # Note that this is the exact same function as presented above just with alternate procedures\n",
        "        # depending on the bidirectional and conjugate symmetry arguments\n",
        "\n",
        "        A_elements = self._state_matrix * jnp.ones((inputs.shape[0], self._state_matrix.shape[0]))\n",
        "        Bu_elements = jax.vmap(lambda u: self._input_matrix @ u)(inputs)\n",
        "\n",
        "        _, xs = jax.lax.associative_scan(binary_operator, (A_elements, Bu_elements))\n",
        "\n",
        "        if self.bidirectional:\n",
        "            _, xs2 = jax.lax.associative_scan(binary_operator,\n",
        "                                          (A_elements, Bu_elements),\n",
        "                                          reverse=True)\n",
        "            xs = jnp.concatenate((xs, xs2), axis=-1)\n",
        "\n",
        "        if self.conj_sym:\n",
        "            ys = jax.vmap(lambda x: 2*(self._output_matrix @ x).real)(xs)\n",
        "        else:\n",
        "            ys = jax.vmap(lambda x: (self._output_matrix @ x).real)(xs)\n",
        "\n",
        "        Du = jax.vmap(lambda u: self._D * u)(inputs)\n",
        "\n",
        "        return ys + Du"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n",
          "is_executing": true
        },
        "id": "pu0NZItog6tD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There we have it, a complete S5 layer! Now let's form a block around it using a structure very similar to a transformer block with a Gated Linear Unit (GLU)."
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "BwSd9kcOg6tD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "outputs": [],
      "source": [
        "import dataclasses\n",
        "\n",
        "@dataclasses.dataclass\n",
        "class S5Block(hk.Module):\n",
        "    ssm: S5\n",
        "    d_model: int\n",
        "    dropout_rate: float\n",
        "    prenorm: bool\n",
        "    istraining: bool = True\n",
        "    name: Optional[str] = None\n",
        "\n",
        "    def __post_init__(self):\n",
        "        super(S5Block, self).__post_init__()\n",
        "        # We could use either layer norm or batch norm.\n",
        "        self._norm = hk.LayerNorm(axis=-1, create_scale=True, create_offset=True)\n",
        "        self._linear = hk.Linear(self.d_model)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        skip = x\n",
        "        if self.prenorm:\n",
        "            x = self._norm(x)\n",
        "\n",
        "        x = self.ssm(x)\n",
        "        # There are a couple of other GLU patterns we could use here, but once again I have chosen\n",
        "        # one semi-arbitrarily to avoid cluttering our module with if statements.\n",
        "        x1 = hk.dropout(hk.next_rng_key(), self.dropout_rate, jax.nn.gelu(x))\n",
        "        x = x * jax.nn.sigmoid(self._linear(x1))\n",
        "        x = hk.dropout(hk.next_rng_key(), self.dropout_rate, x)\n",
        "\n",
        "        x = skip + x\n",
        "        if not self.prenorm:\n",
        "            x = self._norm(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n",
          "is_executing": true
        },
        "id": "p9oMO9Tfg6tD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's make a stack of these blocks:"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "nxrqTJJXg6tE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "outputs": [],
      "source": [
        "@dataclasses.dataclass\n",
        "class S5Stack(hk.Module):\n",
        "    ssm: S5\n",
        "    d_model: int\n",
        "    n_layers: int\n",
        "    dropout_rate: float\n",
        "    prenorm: bool\n",
        "    istraining: bool = True\n",
        "    name: Optional[str] = None\n",
        "\n",
        "    def __post_init__(self):\n",
        "        super(S5Stack, self).__post_init__(name=self.name)\n",
        "        self._encoder = hk.Linear(self.d_model)\n",
        "        self._layers = [\n",
        "            S5Block(\n",
        "                ssm=self.ssm,\n",
        "                d_model=self.d_model,\n",
        "                dropout_rate=self.dropout_rate,\n",
        "                istraining=self.istraining,\n",
        "                prenorm=self.prenorm,\n",
        "            )\n",
        "            for _ in range(self.n_layers)\n",
        "        ]\n",
        "\n",
        "    def __call__(self, x):\n",
        "        x = self._encoder(x)\n",
        "        for layer in self._layers:\n",
        "            x = layer(x)\n",
        "        return x"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n",
          "is_executing": true
        },
        "id": "Sa0yRsNsg6tE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementing Regression in S5"
      ],
      "metadata": {
        "id": "uAD9a1Oyr2Hr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a new dataset class to hold our stock data"
      ],
      "metadata": {
        "id": "zFLCFHwusNTm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- New Data Section ---\n",
        "\n",
        "# This custom Dataset class will hold our sliding windows\n",
        "class StockWindowDataset(TorchDataset):\n",
        "    def __init__(self, data, labels):\n",
        "        # Explicitly convert JAX array -> NumPy array -> Torch tensor\n",
        "        self.data = torch.tensor(np.array(data), dtype=torch.float32)\n",
        "        self.labels = torch.tensor(np.array(labels), dtype=torch.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # We need to add an extra dimension for the input feature\n",
        "        return self.data[idx][..., None], self.labels[idx]\n",
        "\n",
        "def create_stock_dataset(batch_size=64, seq_length=7):\n",
        "    print(\"[*] Generating Stock Price Dataset...\")\n",
        "\n",
        "    # 1. Download data\n",
        "    # Let's use the S&P 500 index as an example\n",
        "    ticker = \"^GSPC\"\n",
        "    data = yf.download(ticker, start=\"2000-01-01\", end=\"2023-12-31\")\n",
        "\n",
        "    # We'll use 'Close' as the representative \"average\" price\n",
        "    prices = data[['Close']].values\n",
        "\n",
        "    # 2. Normalise the data\n",
        "    # We fit the scaler ONLY on the training data to prevent data leakage\n",
        "    train_split_idx = int(len(prices) * 0.8)\n",
        "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "\n",
        "    # Fit on training data\n",
        "    scaler.fit(prices[:train_split_idx])\n",
        "\n",
        "    # Transform all data\n",
        "    prices_scaled = scaler.transform(prices).flatten() # Flatten to 1D array\n",
        "\n",
        "    # 3. Create sliding windows\n",
        "    X, y = [], []\n",
        "    for i in range(len(prices_scaled) - seq_length):\n",
        "        X.append(prices_scaled[i : i + seq_length])\n",
        "        y.append(prices_scaled[i + seq_length])\n",
        "\n",
        "    X = jnp.array(X)\n",
        "    y = jnp.array(y)\n",
        "\n",
        "    # 4. Split into train and test sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, shuffle=False # Time-series data should not be shuffled\n",
        "    )\n",
        "\n",
        "    print(f\"[*] Train samples: {len(X_train)}, Test samples: {len(X_test)}\")\n",
        "\n",
        "    # 5. Create DataLoaders\n",
        "    train_dataset = StockWindowDataset(X_train, y_train)\n",
        "    test_dataset = StockWindowDataset(X_test, y_test)\n",
        "\n",
        "    trainloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    testloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # N_CLASSES is now 1 (we predict one value)\n",
        "    # SEQ_LENGTH is our window size (7)\n",
        "    # IN_DIM is 1 (just the price)\n",
        "    N_CLASSES = 1\n",
        "    IN_DIM = 1\n",
        "\n",
        "    # We also return the scaler so we can invert the predictions later\n",
        "    return trainloader, testloader, N_CLASSES, seq_length, IN_DIM, scaler\n",
        "\n",
        "# Replace the Datasets dict and create_dataset function\n",
        "Datasets = {\n",
        "    \"stock-prediction\": create_stock_dataset,\n",
        "}\n",
        "\n",
        "# Simple class for storing our dataset parameters\n",
        "class Dataset(NamedTuple):\n",
        "    trainloader: DataLoader\n",
        "    testloader: DataLoader\n",
        "    n_classes: int  # This will be 1\n",
        "    seq_length: int # This will be 7\n",
        "    d_input: int    # This will be 1\n",
        "    scaler: MinMaxScaler # Store the scaler\n",
        "    classification: bool\n",
        "\n",
        "def create_dataset(dataset: str, batch_size: int, seq_length: int) -> Dataset:\n",
        "    classification = 'classification' in dataset\n",
        "    dataset_init = Datasets[dataset]\n",
        "\n",
        "    # Pass seq_length to the function\n",
        "    trainloader, testloader, n_classes, seq_len, d_input, scaler = dataset_init(\n",
        "        batch_size=batch_size, seq_length=seq_length\n",
        "    )\n",
        "\n",
        "    return Dataset(\n",
        "        trainloader,\n",
        "        testloader,\n",
        "        n_classes,\n",
        "        seq_len,\n",
        "        d_input,\n",
        "        scaler, # Add scaler here\n",
        "        classification\n",
        "    )"
      ],
      "metadata": {
        "id": "5dl5NtHfr7Cv"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating our Regressor"
      ],
      "metadata": {
        "id": "UzZvMnv4whWZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace S5Classifier with S5Regressor\n",
        "\n",
        "@dataclasses.dataclass\n",
        "class S5Regressor(hk.Module):\n",
        "    ssm: S5\n",
        "    d_model: int\n",
        "    d_output: int  # This will be 1 for our task\n",
        "    n_layers: int\n",
        "    dropout_rate: float\n",
        "    mode: str = 'last'  # 'last' is more common for time-series prediction\n",
        "    prenorm: bool = True\n",
        "    istraining: bool = True\n",
        "    name: Optional[str] = None\n",
        "\n",
        "    def __post_init__(self):\n",
        "        super(S5Regressor, self).__post_init__(name=self.name)\n",
        "        self._encoder = S5Stack(\n",
        "            ssm=self.ssm,\n",
        "            d_model=self.d_model,\n",
        "            n_layers=self.n_layers,\n",
        "            dropout_rate=self.dropout_rate,\n",
        "            istraining=self.istraining,\n",
        "            prenorm=self.prenorm,\n",
        "        )\n",
        "        self._decoder = hk.Linear(self.d_output)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        x = self._encoder(x)\n",
        "\n",
        "        # For predicting the next step, using the 'last' hidden state\n",
        "        # is usually the best approach.\n",
        "        if self.mode == 'pool':\n",
        "            x = jnp.mean(x, axis=0)\n",
        "        elif self.mode == 'last':\n",
        "            x = x[-1] # Take the last element of the sequence\n",
        "        else:\n",
        "            raise NotImplementedError(\"Mode must be in ['pool', 'last]\")\n",
        "\n",
        "        x = self._decoder(x)\n",
        "\n",
        "        # CRITICAL: Remove the log_softmax! We want the raw output value.\n",
        "        # We also squeeze the last dimension to match the label shape (batch_size,)\n",
        "        return jnp.squeeze(x, axis=-1)"
      ],
      "metadata": {
        "id": "I-5OkcXLspHI"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we will set some hyperparameters:"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "Oen4DWzdg6tE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "outputs": [],
      "source": [
        "# --- Set Hyperparameters ---\n",
        "# These are a good start, but you will need to tune them for this new task.\n",
        "STATE_SIZE: int = 64   # Reduced for a simpler 1D input\n",
        "D_MODEL: int = 32    # Reduced\n",
        "N_LAYERS: int = 4\n",
        "N_BLOCKS: int = 4\n",
        "EPOCHS: int = 50\n",
        "BATCH_SIZE: int = 64\n",
        "DROPOUT_RATE: float = 0.1\n",
        "LEARNING_RATE: float = 1e-3\n",
        "SEED = 0\n",
        "SEQ_LENGTH: int = 7  # Your specified 7-day window"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n",
          "is_executing": true
        },
        "id": "5RsrP52qg6tE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To keep things simple for this example, we will just be using a plain adam optimizer, but the results can be highly improved with extra techniques such as cosine annealing, learning rate schedules and weight decay."
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "URrXKlSyg6tE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "outputs": [],
      "source": [
        "import optax\n",
        "\n",
        "class TrainingState(NamedTuple):\n",
        "    params: hk.Params\n",
        "    opt_state: optax.OptState\n",
        "    rng_key: jnp.ndarray\n",
        "\n",
        "optim = optax.adam(LEARNING_RATE)"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n",
          "is_executing": true
        },
        "id": "838zDBvUg6tE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Now for the loss and update functions:"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "Egd1GpRPg6tF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "outputs": [],
      "source": [
        "from functools import partial\n",
        "from typing import Tuple, MutableMapping, Any\n",
        "_Metrics = MutableMapping[str, Any]\n",
        "\n",
        "# Replace cross_entropy_loss and compute_accuracy\n",
        "\n",
        "@partial(jnp.vectorize, signature=\"(),()->()\")\n",
        "def mse_loss(prediction, target) -> jnp.ndarray:\n",
        "    \"\"\"Computes Mean Squared Error loss.\"\"\"\n",
        "    return (prediction - target) ** 2\n",
        "\n",
        "# We no longer need compute_accuracy\n",
        "\n",
        "# --- Modify the 'update' function ---\n",
        "@partial(jax.jit, static_argnums=(3, 4))\n",
        "def update(\n",
        "        state: TrainingState,\n",
        "        inputs: jnp.ndarray,\n",
        "        targets: jnp.ndarray,\n",
        "        model: hk.transform,\n",
        "        classification: bool,) -> Tuple[TrainingState, _Metrics]:\n",
        "\n",
        "    rng_key, next_rng_key = jax.random.split(state.rng_key)\n",
        "\n",
        "    def loss_fn(params):\n",
        "        # 'predictions' instead of 'logits'\n",
        "        predictions = model.apply(params, rng_key, inputs)\n",
        "\n",
        "        # Use MSE loss\n",
        "        _loss = jnp.mean(mse_loss(predictions, targets))\n",
        "\n",
        "        # We return loss as the primary value and as an aux value\n",
        "        return _loss, _loss\n",
        "\n",
        "    # This 'if' is no longer needed, but harmless if left.\n",
        "    # if not classification:\n",
        "    #     targets = inputs[:, :, 0]\n",
        "\n",
        "    grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
        "\n",
        "    # (loss, loss_metric), gradients\n",
        "    (loss, _), gradients = grad_fn(state.params)\n",
        "\n",
        "    updates, new_opt_state = optim.update(gradients, state.opt_state, state.params)\n",
        "    new_params = optax.apply_updates(state.params, updates)\n",
        "\n",
        "    new_state = TrainingState(\n",
        "        params=new_params,\n",
        "        opt_state=new_opt_state,\n",
        "        rng_key=next_rng_key,\n",
        "    )\n",
        "    metrics = {\n",
        "        'loss': loss, # We only track loss now\n",
        "    }\n",
        "\n",
        "    return new_state, metrics\n",
        "\n",
        "# --- Modify the 'evaluate' function ---\n",
        "@partial(jax.jit, static_argnums=(3, 4))\n",
        "def evaluate(\n",
        "        state: TrainingState,\n",
        "        inputs: jnp.ndarray,\n",
        "        targets: jnp.ndarray,\n",
        "        model: hk.transform,\n",
        "        classification,) -> _Metrics:\n",
        "\n",
        "    rng_key, _ = jax.random.split(state.rng_key, 2)\n",
        "\n",
        "    # if not classification:\n",
        "    #     targets = inputs[:, :, 0]\n",
        "\n",
        "    predictions = model.apply(state.params, rng_key, inputs)\n",
        "    loss = jnp.mean(mse_loss(predictions, targets))\n",
        "\n",
        "    metrics = {\n",
        "        'loss': loss, # We only track loss\n",
        "    }\n",
        "\n",
        "    return metrics"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n",
          "is_executing": true
        },
        "id": "VHh5Id9fg6tF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Now we call these update and evaluate functions in their respective epochs:"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "2nCPmxiWg6tF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "# --- Modify 'training_epoch' ---\n",
        "def training_epoch(\n",
        "        state: TrainingState,\n",
        "        trainloader: DataLoader,\n",
        "        model: hk.transform,\n",
        "        classification: bool = False,) -> Tuple[TrainingState, jnp.ndarray]: # Removed accuracy output\n",
        "\n",
        "    batch_losses = [] # Removed batch_accuracies\n",
        "    for batch_idx, (inputs, targets) in enumerate(tqdm(trainloader)):\n",
        "        inputs = jnp.array(inputs.numpy())\n",
        "        targets = jnp.array(targets.numpy())\n",
        "        state, metrics = update(\n",
        "            state, inputs, targets,\n",
        "            model, classification\n",
        "        )\n",
        "        batch_losses.append(metrics['loss'])\n",
        "        # Removed accuracy\n",
        "\n",
        "    return (\n",
        "        state,\n",
        "        jnp.mean(jnp.array(batch_losses)),\n",
        "        # Removed accuracy\n",
        "    )\n",
        "\n",
        "# --- Modify 'validation_epoch' ---\n",
        "def validation_epoch(\n",
        "    state: TrainingState,\n",
        "    testloader: DataLoader,\n",
        "    model: hk.transform,\n",
        "    classification: bool = True,) -> jnp.ndarray: # Removed accuracy output\n",
        "\n",
        "    losses = [] # Removed accuracies\n",
        "    for batch_idx, (inputs, targets) in enumerate(tqdm(testloader)):\n",
        "        inputs = jnp.array(inputs.numpy())\n",
        "        targets = jnp.array(targets.numpy())\n",
        "        metrics = evaluate(\n",
        "            state, inputs, targets,\n",
        "            model, classification\n",
        "        )\n",
        "        losses.append(metrics['loss'])\n",
        "        # Removed accuracy\n",
        "\n",
        "    return jnp.mean(jnp.array(losses)) # Removed accuracy"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n",
          "is_executing": true
        },
        "id": "OXJcCJnvg6tF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "outputs": [],
      "source": [
        "import torch\n",
        "# Set random number generators\n",
        "torch.random.manual_seed(SEED)\n",
        "key = jax.random.PRNGKey(SEED)\n",
        "rng, init_rng = jax.random.split(key)"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n",
          "is_executing": true
        },
        "id": "undnZLH7g6tF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3393255481.py:23: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  data = yf.download(ticker, start=\"2000-01-01\", end=\"2023-12-31\")\n",
            "\r[*********************100%***********************]  1 of 1 completed"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[*] Generating Stock Price Dataset...\n",
            "[*] Train samples: 4824, Test samples: 1206\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Note: We now pass SEQ_LENGTH\n",
        "ds = create_dataset(\n",
        "    'stock-prediction',\n",
        "    BATCH_SIZE,\n",
        "    seq_length=SEQ_LENGTH\n",
        ")\n",
        "init_data = jnp.array(next(iter(ds.trainloader))[0].numpy())"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n",
          "is_executing": true
        },
        "id": "m_M81cNFg6tF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55f91729-af28-4adc-e125-e1a9ae314025"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "outputs": [],
      "source": [
        "# In Haiku, we have to call our model inside a transformed function using hk.transform for it to become\n",
        "# functionally pure and compatible with essential JAX functions like jax.grad(). Here we are using hk.vmap\n",
        "# instead of jax.vmap because we are calling it from within a hk.transform.\n",
        "@hk.transform\n",
        "def forward(x) -> hk.transform:\n",
        "    # Use S5Regressor\n",
        "    neural_net = S5Regressor(\n",
        "        S5(\n",
        "            STATE_SIZE,\n",
        "            D_MODEL,\n",
        "            N_BLOCKS,\n",
        "        ),\n",
        "        D_MODEL,\n",
        "        ds.n_classes,  # This will be 1\n",
        "        N_LAYERS,\n",
        "        DROPOUT_RATE,\n",
        "    )\n",
        "    return hk.vmap(neural_net, split_rng=False)(x)"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n",
          "is_executing": true
        },
        "id": "urrTfYtEg6tF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jax/_src/lax/lax.py:1660: ComplexWarning: Casting complex values to real discards the imaginary part\n",
            "  return _convert_element_type(operand, new_dtype, weak_type=False)  # type: ignore[unused-ignore,bad-return-type]\n"
          ]
        }
      ],
      "source": [
        "# Set state\n",
        "initial_params = forward.init(init_rng, init_data)\n",
        "initial_opt_state = optim.init(initial_params)\n",
        "\n",
        "state = TrainingState(\n",
        "    params=initial_params,\n",
        "    opt_state=initial_opt_state,\n",
        "    rng_key=rng)"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n",
          "is_executing": true
        },
        "id": "-daPDGdlg6tF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93ad8e65-8633-43d6-b994-458fcefd6b3c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training and Plotting"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "7yFEQtQvg6tG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[*] Training Epoch 1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/76 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/jax/_src/lax/lax.py:5473: ComplexWarning: Casting complex values to real discards the imaginary part\n",
            "  x_bar = _convert_element_type(x_bar, x.aval.dtype, x.aval.weak_type)\n",
            "100%|██████████| 76/76 [00:18<00:00,  4.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[*] Running Epoch 1 Validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 19/19 [00:09<00:00,  2.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=>> Epoch 1 Metrics ===\n",
            "\tTrain Loss (MSE): 2.65943\n",
            "\t Test Loss (MSE): 1.46223\n",
            "[*] Training Epoch 2...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 76/76 [00:00<00:00, 330.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[*] Running Epoch 2 Validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 19/19 [00:00<00:00, 559.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=>> Epoch 2 Metrics ===\n",
            "\tTrain Loss (MSE): 0.96576\n",
            "\t Test Loss (MSE): 0.26262\n",
            "[*] Training Epoch 3...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 76/76 [00:00<00:00, 365.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[*] Running Epoch 3 Validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 19/19 [00:00<00:00, 575.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=>> Epoch 3 Metrics ===\n",
            "\tTrain Loss (MSE): 0.65894\n",
            "\t Test Loss (MSE): 1.07399\n",
            "[*] Training Epoch 4...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 76/76 [00:00<00:00, 377.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[*] Running Epoch 4 Validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 19/19 [00:00<00:00, 567.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=>> Epoch 4 Metrics ===\n",
            "\tTrain Loss (MSE): 0.46738\n",
            "\t Test Loss (MSE): 0.03111\n",
            "[*] Training Epoch 5...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 76/76 [00:00<00:00, 372.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[*] Running Epoch 5 Validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 19/19 [00:00<00:00, 579.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=>> Epoch 5 Metrics ===\n",
            "\tTrain Loss (MSE): 0.43795\n",
            "\t Test Loss (MSE): 0.13936\n",
            "[*] Training Epoch 6...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 76/76 [00:00<00:00, 336.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[*] Running Epoch 6 Validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 19/19 [00:00<00:00, 555.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=>> Epoch 6 Metrics ===\n",
            "\tTrain Loss (MSE): 0.27025\n",
            "\t Test Loss (MSE): 0.13029\n",
            "[*] Training Epoch 7...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 76/76 [00:00<00:00, 356.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[*] Running Epoch 7 Validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 19/19 [00:00<00:00, 556.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=>> Epoch 7 Metrics ===\n",
            "\tTrain Loss (MSE): 0.27289\n",
            "\t Test Loss (MSE): 0.61472\n",
            "[*] Training Epoch 8...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 76/76 [00:00<00:00, 364.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[*] Running Epoch 8 Validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 19/19 [00:00<00:00, 530.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=>> Epoch 8 Metrics ===\n",
            "\tTrain Loss (MSE): 0.14729\n",
            "\t Test Loss (MSE): 0.05707\n",
            "[*] Training Epoch 9...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 76/76 [00:00<00:00, 359.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[*] Running Epoch 9 Validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 19/19 [00:00<00:00, 414.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=>> Epoch 9 Metrics ===\n",
            "\tTrain Loss (MSE): 0.11345\n",
            "\t Test Loss (MSE): 0.60465\n",
            "[*] Training Epoch 10...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 76/76 [00:00<00:00, 364.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[*] Running Epoch 10 Validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 19/19 [00:00<00:00, 581.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=>> Epoch 10 Metrics ===\n",
            "\tTrain Loss (MSE): 0.10805\n",
            "\t Test Loss (MSE): 0.16906\n",
            "[*] Training Epoch 11...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 76/76 [00:00<00:00, 369.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[*] Running Epoch 11 Validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 19/19 [00:00<00:00, 560.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=>> Epoch 11 Metrics ===\n",
            "\tTrain Loss (MSE): 0.12195\n",
            "\t Test Loss (MSE): 0.10933\n",
            "[*] Training Epoch 12...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 76/76 [00:00<00:00, 348.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[*] Running Epoch 12 Validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 19/19 [00:00<00:00, 549.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=>> Epoch 12 Metrics ===\n",
            "\tTrain Loss (MSE): 0.09899\n",
            "\t Test Loss (MSE): 0.07827\n",
            "[*] Training Epoch 13...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 76/76 [00:00<00:00, 337.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[*] Running Epoch 13 Validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 19/19 [00:00<00:00, 551.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=>> Epoch 13 Metrics ===\n",
            "\tTrain Loss (MSE): 0.08241\n",
            "\t Test Loss (MSE): 0.00561\n",
            "[*] Training Epoch 14...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 76/76 [00:00<00:00, 349.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[*] Running Epoch 14 Validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 19/19 [00:00<00:00, 578.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=>> Epoch 14 Metrics ===\n",
            "\tTrain Loss (MSE): 0.05938\n",
            "\t Test Loss (MSE): 0.01048\n",
            "[*] Training Epoch 15...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 76/76 [00:00<00:00, 361.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[*] Running Epoch 15 Validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 19/19 [00:00<00:00, 534.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=>> Epoch 15 Metrics ===\n",
            "\tTrain Loss (MSE): 0.06950\n",
            "\t Test Loss (MSE): 0.24101\n",
            "[*] Training Epoch 16...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 76/76 [00:00<00:00, 351.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[*] Running Epoch 16 Validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 19/19 [00:00<00:00, 574.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=>> Epoch 16 Metrics ===\n",
            "\tTrain Loss (MSE): 0.05916\n",
            "\t Test Loss (MSE): 0.11834\n",
            "[*] Training Epoch 17...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 76/76 [00:00<00:00, 340.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[*] Running Epoch 17 Validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 19/19 [00:00<00:00, 560.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=>> Epoch 17 Metrics ===\n",
            "\tTrain Loss (MSE): 0.06413\n",
            "\t Test Loss (MSE): 0.18363\n",
            "[*] Training Epoch 18...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 76/76 [00:00<00:00, 277.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[*] Running Epoch 18 Validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 19/19 [00:00<00:00, 414.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=>> Epoch 18 Metrics ===\n",
            "\tTrain Loss (MSE): 0.03331\n",
            "\t Test Loss (MSE): 0.00855\n",
            "[*] Training Epoch 19...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 76/76 [00:00<00:00, 246.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[*] Running Epoch 19 Validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 19/19 [00:00<00:00, 382.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=>> Epoch 19 Metrics ===\n",
            "\tTrain Loss (MSE): 0.04843\n",
            "\t Test Loss (MSE): 0.00143\n",
            "[*] Training Epoch 20...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 76/76 [00:00<00:00, 260.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[*] Running Epoch 20 Validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 19/19 [00:00<00:00, 419.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=>> Epoch 20 Metrics ===\n",
            "\tTrain Loss (MSE): 0.04799\n",
            "\t Test Loss (MSE): 0.07406\n",
            "[*] Training Epoch 21...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 76/76 [00:00<00:00, 261.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[*] Running Epoch 21 Validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 19/19 [00:00<00:00, 413.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=>> Epoch 21 Metrics ===\n",
            "\tTrain Loss (MSE): 0.04352\n",
            "\t Test Loss (MSE): 0.03603\n",
            "[*] Training Epoch 22...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 76/76 [00:00<00:00, 257.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[*] Running Epoch 22 Validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 19/19 [00:00<00:00, 383.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=>> Epoch 22 Metrics ===\n",
            "\tTrain Loss (MSE): 0.04145\n",
            "\t Test Loss (MSE): 0.09056\n",
            "[*] Training Epoch 23...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 76/76 [00:00<00:00, 233.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[*] Running Epoch 23 Validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 19/19 [00:00<00:00, 375.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=>> Epoch 23 Metrics ===\n",
            "\tTrain Loss (MSE): 0.03797\n",
            "\t Test Loss (MSE): 0.04545\n",
            "[*] Training Epoch 24...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 76/76 [00:00<00:00, 337.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[*] Running Epoch 24 Validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 19/19 [00:00<00:00, 545.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=>> Epoch 24 Metrics ===\n",
            "\tTrain Loss (MSE): 0.03318\n",
            "\t Test Loss (MSE): 0.18809\n",
            "[*] Training Epoch 25...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 76/76 [00:00<00:00, 367.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[*] Running Epoch 25 Validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 19/19 [00:00<00:00, 566.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=>> Epoch 25 Metrics ===\n",
            "\tTrain Loss (MSE): 0.03785\n",
            "\t Test Loss (MSE): 0.00729\n",
            "[*] Training Epoch 26...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 76/76 [00:00<00:00, 341.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[*] Running Epoch 26 Validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 19/19 [00:00<00:00, 529.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=>> Epoch 26 Metrics ===\n",
            "\tTrain Loss (MSE): 0.02569\n",
            "\t Test Loss (MSE): 0.07025\n",
            "[*] Training Epoch 27...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 76/76 [00:00<00:00, 347.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[*] Running Epoch 27 Validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 19/19 [00:00<00:00, 533.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=>> Epoch 27 Metrics ===\n",
            "\tTrain Loss (MSE): 0.01904\n",
            "\t Test Loss (MSE): 0.00063\n",
            "[*] Training Epoch 28...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 76/76 [00:00<00:00, 350.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[*] Running Epoch 28 Validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 19/19 [00:00<00:00, 563.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=>> Epoch 28 Metrics ===\n",
            "\tTrain Loss (MSE): 0.02409\n",
            "\t Test Loss (MSE): 0.03122\n",
            "[*] Training Epoch 29...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 76/76 [00:00<00:00, 367.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[*] Running Epoch 29 Validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 19/19 [00:00<00:00, 546.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=>> Epoch 29 Metrics ===\n",
            "\tTrain Loss (MSE): 0.03161\n",
            "\t Test Loss (MSE): 0.00120\n",
            "[*] Training Epoch 30...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 76/76 [00:00<00:00, 335.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[*] Running Epoch 30 Validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 19/19 [00:00<00:00, 531.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=>> Epoch 30 Metrics ===\n",
            "\tTrain Loss (MSE): 0.01779\n",
            "\t Test Loss (MSE): 0.01117\n",
            "[*] Training Epoch 31...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 76/76 [00:00<00:00, 359.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[*] Running Epoch 31 Validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 19/19 [00:00<00:00, 538.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=>> Epoch 31 Metrics ===\n",
            "\tTrain Loss (MSE): 0.02065\n",
            "\t Test Loss (MSE): 0.01526\n",
            "[*] Training Epoch 32...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 76/76 [00:00<00:00, 357.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[*] Running Epoch 32 Validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 19/19 [00:00<00:00, 558.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=>> Epoch 32 Metrics ===\n",
            "\tTrain Loss (MSE): 0.01950\n",
            "\t Test Loss (MSE): 0.01762\n",
            "[*] Training Epoch 33...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 76/76 [00:00<00:00, 340.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[*] Running Epoch 33 Validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 19/19 [00:00<00:00, 546.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=>> Epoch 33 Metrics ===\n",
            "\tTrain Loss (MSE): 0.01507\n",
            "\t Test Loss (MSE): 0.00111\n",
            "[*] Training Epoch 34...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 76/76 [00:00<00:00, 365.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[*] Running Epoch 34 Validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 19/19 [00:00<00:00, 536.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=>> Epoch 34 Metrics ===\n",
            "\tTrain Loss (MSE): 0.02173\n",
            "\t Test Loss (MSE): 0.02188\n",
            "[*] Training Epoch 35...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 76/76 [00:00<00:00, 362.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[*] Running Epoch 35 Validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 19/19 [00:00<00:00, 558.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=>> Epoch 35 Metrics ===\n",
            "\tTrain Loss (MSE): 0.01269\n",
            "\t Test Loss (MSE): 0.00461\n",
            "[*] Training Epoch 36...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 76/76 [00:00<00:00, 366.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[*] Running Epoch 36 Validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 19/19 [00:00<00:00, 556.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=>> Epoch 36 Metrics ===\n",
            "\tTrain Loss (MSE): 0.01207\n",
            "\t Test Loss (MSE): 0.00287\n",
            "[*] Training Epoch 37...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 76/76 [00:00<00:00, 318.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[*] Running Epoch 37 Validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 19/19 [00:00<00:00, 554.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=>> Epoch 37 Metrics ===\n",
            "\tTrain Loss (MSE): 0.01512\n",
            "\t Test Loss (MSE): 0.00296\n",
            "[*] Training Epoch 38...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 76/76 [00:00<00:00, 350.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[*] Running Epoch 38 Validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 19/19 [00:00<00:00, 536.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=>> Epoch 38 Metrics ===\n",
            "\tTrain Loss (MSE): 0.01288\n",
            "\t Test Loss (MSE): 0.01814\n",
            "[*] Training Epoch 39...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 76/76 [00:00<00:00, 352.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[*] Running Epoch 39 Validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 19/19 [00:00<00:00, 540.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=>> Epoch 39 Metrics ===\n",
            "\tTrain Loss (MSE): 0.01319\n",
            "\t Test Loss (MSE): 0.00710\n",
            "[*] Training Epoch 40...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 76/76 [00:00<00:00, 367.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[*] Running Epoch 40 Validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 19/19 [00:00<00:00, 546.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=>> Epoch 40 Metrics ===\n",
            "\tTrain Loss (MSE): 0.01592\n",
            "\t Test Loss (MSE): 0.01143\n",
            "[*] Training Epoch 41...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 76/76 [00:00<00:00, 343.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[*] Running Epoch 41 Validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 19/19 [00:00<00:00, 554.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=>> Epoch 41 Metrics ===\n",
            "\tTrain Loss (MSE): 0.01762\n",
            "\t Test Loss (MSE): 0.00891\n",
            "[*] Training Epoch 42...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 76/76 [00:00<00:00, 352.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[*] Running Epoch 42 Validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 19/19 [00:00<00:00, 556.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=>> Epoch 42 Metrics ===\n",
            "\tTrain Loss (MSE): 0.01210\n",
            "\t Test Loss (MSE): 0.01255\n",
            "[*] Training Epoch 43...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 76/76 [00:00<00:00, 371.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[*] Running Epoch 43 Validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 19/19 [00:00<00:00, 558.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=>> Epoch 43 Metrics ===\n",
            "\tTrain Loss (MSE): 0.00951\n",
            "\t Test Loss (MSE): 0.00464\n",
            "[*] Training Epoch 44...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 76/76 [00:00<00:00, 340.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[*] Running Epoch 44 Validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 19/19 [00:00<00:00, 533.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=>> Epoch 44 Metrics ===\n",
            "\tTrain Loss (MSE): 0.01032\n",
            "\t Test Loss (MSE): 0.01130\n",
            "[*] Training Epoch 45...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 76/76 [00:00<00:00, 350.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[*] Running Epoch 45 Validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 19/19 [00:00<00:00, 512.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=>> Epoch 45 Metrics ===\n",
            "\tTrain Loss (MSE): 0.00813\n",
            "\t Test Loss (MSE): 0.00094\n",
            "[*] Training Epoch 46...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 76/76 [00:00<00:00, 348.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[*] Running Epoch 46 Validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 19/19 [00:00<00:00, 538.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=>> Epoch 46 Metrics ===\n",
            "\tTrain Loss (MSE): 0.00901\n",
            "\t Test Loss (MSE): 0.01361\n",
            "[*] Training Epoch 47...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 76/76 [00:00<00:00, 358.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[*] Running Epoch 47 Validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 19/19 [00:00<00:00, 546.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=>> Epoch 47 Metrics ===\n",
            "\tTrain Loss (MSE): 0.01092\n",
            "\t Test Loss (MSE): 0.00128\n",
            "[*] Training Epoch 48...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 76/76 [00:00<00:00, 336.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[*] Running Epoch 48 Validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 19/19 [00:00<00:00, 545.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=>> Epoch 48 Metrics ===\n",
            "\tTrain Loss (MSE): 0.00868\n",
            "\t Test Loss (MSE): 0.00792\n",
            "[*] Training Epoch 49...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 76/76 [00:00<00:00, 349.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[*] Running Epoch 49 Validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 19/19 [00:00<00:00, 533.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=>> Epoch 49 Metrics ===\n",
            "\tTrain Loss (MSE): 0.01074\n",
            "\t Test Loss (MSE): 0.06159\n",
            "[*] Training Epoch 50...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 76/76 [00:00<00:00, 349.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[*] Running Epoch 50 Validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 19/19 [00:00<00:00, 538.86it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=>> Epoch 50 Metrics ===\n",
            "\tTrain Loss (MSE): 0.00634\n",
            "\t Test Loss (MSE): 0.00110\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(EPOCHS):\n",
        "    print(f\"[*] Training Epoch {epoch + 1}...\")\n",
        "    # Update function signatures\n",
        "    state, training_loss = training_epoch(\n",
        "        state,\n",
        "        ds.trainloader,\n",
        "        forward,\n",
        "        ds.classification\n",
        "    )\n",
        "    print(f\"[*] Running Epoch {epoch + 1} Validation...\")\n",
        "    # Update function signatures\n",
        "    test_loss = validation_epoch(\n",
        "        state,\n",
        "        ds.testloader,\n",
        "        forward,\n",
        "        ds.classification\n",
        "    )\n",
        "\n",
        "    # Updated print statement\n",
        "    print(f\"\\n=>> Epoch {epoch + 1} Metrics ===\")\n",
        "    print(\n",
        "        f\"\\tTrain Loss (MSE): {training_loss:.5f}\\n\\t Test Loss (MSE): {test_loss:.5f}\"\n",
        "    )"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "LEWQ9Rbig6tG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb572267-0a04-411d-f404-d8c4d3e6b8b7"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}