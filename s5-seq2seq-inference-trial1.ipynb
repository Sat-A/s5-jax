{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sat-A/s5-jax/blob/main/s5-seq2seq-inference-trial1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# S5-Predictor"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "1YMG3-5Hg6s_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The goal of this exercise is to adapt an annotated implementation of S5 https://github.com/JPGoodale/annotated-s5 that was made for classification tasks to regression tasks."
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "dXRZLfGeg6tA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Runtime Setup\n",
        "Ensure runtime is set to GPU to ensure gpu parallelisation speedup"
      ],
      "metadata": {
        "id": "WIpj_s_ZcaAJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dm-haiku\n",
        "!pip install hippox"
      ],
      "metadata": {
        "id": "XylzIGnbooEy",
        "outputId": "0f116944-3995-465c-fea6-49d999203bae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: dm-haiku in /usr/local/lib/python3.12/dist-packages (0.0.15)\n",
            "Requirement already satisfied: absl-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from dm-haiku) (1.4.0)\n",
            "Requirement already satisfied: jmp>=0.0.2 in /usr/local/lib/python3.12/dist-packages (from dm-haiku) (0.0.4)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.12/dist-packages (from dm-haiku) (2.0.2)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.12/dist-packages (from dm-haiku) (0.9.0)\n",
            "Requirement already satisfied: hippox in /usr/local/lib/python3.12/dist-packages (0.0.9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "outputs": [],
      "source": [
        "# Let's go ahead and import hippox and the core JAX libraries we will be using:\n",
        "import jax\n",
        "import numpy as np\n",
        "import jax.numpy as jnp\n",
        "import haiku as hk\n",
        "from hippox.main import Hippo\n",
        "from typing import Optional\n",
        "\n",
        "# New imports for stock data\n",
        "import yfinance as yf\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch  # Make sure torch is imported if not already\n",
        "from torch.utils.data import DataLoader, Dataset as TorchDataset\n",
        "from typing import NamedTuple\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n",
          "is_executing": true
        },
        "id": "YYWAK_LIg6tB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First we'll define some helper functions for discretization and timescale initialization as the SSM equation is naturally continuous and must be made discrete to be unrolled as a linear recurrence like standard RNNs."
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "psmzqs8Xg6tB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "outputs": [],
      "source": [
        "# Here we are just using the zero order hold method for its sheer simplicity, with A, B and delta_t denoting the\n",
        "# state matrix, input matrix and change in timescale respectively.\n",
        "def discretize(A, B, delta_t):\n",
        "    Identity = jnp.ones(A.shape[0])\n",
        "    _A = jnp.exp(A*delta_t)\n",
        "    _B = (1/A * (_A-Identity)) * B\n",
        "    return _A, _B\n",
        "\n",
        "# This is a function used to initialize the trainable timescale parameter.\n",
        "def log_step_initializer(dt_min=0.001, dt_max=0.1):\n",
        "    def init(shape, dtype):\n",
        "        uniform = hk.initializers.RandomUniform()\n",
        "        return uniform(shape, dtype)*(jnp.log(dt_max) - jnp.log(dt_min)) + jnp.log(dt_min)\n",
        "    return init\n",
        "\n",
        "# Taken directly from https://github.com/deepmind/dm-haiku/blob/main/haiku/_src/recurrent.py\n",
        "def add_batch(nest, batch_size: Optional[int]):\n",
        "    broadcast = lambda x: jnp.broadcast_to(x, (batch_size,) + x.shape)\n",
        "    return jax.tree_util.tree_map(broadcast, nest)"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n",
          "is_executing": true
        },
        "id": "Ous4HGubg6tB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The linear SSM equation is as follows:\n",
        "$$ x_0(t) = Ax(t) + Bu(t) $$\n",
        "$$ y(t) = Cx(t) + Du(t) $$\n",
        "\n",
        " We will now implement it as a recurrent Haiku module:"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "SaJ9vnolg6tB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "outputs": [],
      "source": [
        "class LinearSSM(hk.RNNCore):\n",
        "    def __init__(self, state_size: int, name: Optional[str] = None):\n",
        "        super(LinearSSM, self).__init__(name=name)\n",
        "        # We won't get into the basis measure families here, just note that they are basically just the\n",
        "        # type of orthogonal polynomial we initialize with, the scaled Legendre measure (LegS) introduced\n",
        "        # in the original HiPPO paper is pretty much the standard initialization and is what is used in the\n",
        "        # main experiments in the S5 paper. I will also note that the Hippo class uses the diagonal representation\n",
        "        # of the state matrix by default, as this has become the standard in neural SSMs since shown to be\n",
        "        # equally effective as the diagonal plus low rank representation in https://arxiv.org/abs/2203.14343\n",
        "        # and then formalized in https://arxiv.org/abs/2206.11893.\n",
        "\n",
        "        _hippo = Hippo(state_size=state_size, basis_measure='legs')\n",
        "        # Must be called for parameters to be initialized\n",
        "        _hippo()\n",
        "\n",
        "        # We register the real and imaginary components of the state matrix A as separate parameters because\n",
        "        # they will have separate gradients in training, they will be conjoined back together and then discretized\n",
        "        # but this will simply be backpropagated through as a transformation of the lambda real and imaginary\n",
        "        # parameters (lambda is just what we call the diagonalized state matrix).\n",
        "\n",
        "        self._lambda_real = hk.get_parameter(\n",
        "            'lambda_real',\n",
        "            shape=[state_size,],\n",
        "            init=_hippo.lambda_initializer('real')\n",
        "        )\n",
        "        self._lambda_imag = hk.get_parameter(\n",
        "            'lambda_imaginary',\n",
        "            shape=[state_size,],\n",
        "            init=_hippo.lambda_initializer('imaginary')\n",
        "        )\n",
        "        self._A = self._lambda_real + 1j * self._lambda_imag\n",
        "\n",
        "       # For now, these initializations of the input and output matrices B and C match the S4D\n",
        "        # parameterization for demonstration purposes, we will implement the S5 versions later.\n",
        "\n",
        "        self._B = hk.get_parameter(\n",
        "            'B',\n",
        "            shape=[state_size,],\n",
        "            init=_hippo.b_initializer()\n",
        "        )\n",
        "        self._C = hk.get_parameter(\n",
        "            'C',\n",
        "            shape=[state_size, 2],\n",
        "            init=hk.initializers.RandomNormal(stddev=0.5**0.5)\n",
        "        )\n",
        "        self._output_matrix = self._C[..., 0] + 1j * self._C[..., 1]\n",
        "\n",
        "        # This feed-through matrix basically acts as a residual connection.\n",
        "        self._D = hk.get_parameter(\n",
        "            'D',\n",
        "            [1,],\n",
        "            init=jnp.ones,\n",
        "        )\n",
        "\n",
        "        self._delta_t = hk.get_parameter(\n",
        "            'delta_t',\n",
        "            shape=[1,],\n",
        "            init=log_step_initializer()\n",
        "        )\n",
        "        timescale = jnp.exp(self._delta_t)\n",
        "\n",
        "        self._state_matrix, self._input_matrix = discretize(self._A, self._B, timescale)\n",
        "\n",
        "    def __call__(self, inputs, prev_state):\n",
        "        u = inputs[:, jnp.newaxis]\n",
        "        new_state = self._state_matrix @ prev_state + self._input_matrix @ u\n",
        "        y_s = self._output_matrix @ new_state\n",
        "        out = y_s.reshape(-1).real + self._D * u\n",
        "        return out, new_state\n",
        "\n",
        "    def initial_state(self, batch_size: Optional[int] = None):\n",
        "        state = jnp.zeros([self._state_size])\n",
        "        if batch_size is not None:\n",
        "            state = add_batch(state, batch_size)\n",
        "        return state"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n",
          "is_executing": true
        },
        "id": "PySpa6EMg6tC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You may notice that this looks an awful lot like a vanilla RNN cell, just with our special parameterization and without any activations, hence being a linear recurrence. I have initialized it as an instance of Haiku's RNN.Core abstract base class so that it can be unrolled using either the hk.dynamic_unroll or hk.static_unroll functions like any other recurrent module, however, if you are familiar with any of the S4 models you may be noticing that there's something crucial missing here: the convolutional representation. One of the key contributions of the S4 paper was its demonstration that the SSM ODE can be represented as either a linear recurrence, as above, for efficient inference, or as a global convolution for much faster training. That paper and the following papers then go on to present various kernels for efficiently computing this convolution with Fast Fourier Transforms, highly improving the computational efficiency of the model. Then why have we omitted them? Because the S5 architecture which we are about to explore simplifies all this by providing a purely recurrent representation in both training and inference, it does this by using a parallel recurrence that actually looks alot like a convolution itself! From the paper:\n",
        "\n",
        "    \"We use parallel scans to efficiently compute the states of a discretized linear SSM. Given a binary associative operator • (i.e. (a • b) • c = a • (b • c)) and a sequence of L elements [a1, a2, ..., aL], the scan operation (sometimes referred to as all-prefix-sum) returns the sequence [a1, (a1 • a2), ..., (a1 • a2 • ... • aL)].\"\n",
        "\n",
        "Let's see what this looks like in code, taken straight from the original author's implementation:"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "S7kVHuCkg6tC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "outputs": [],
      "source": [
        "@jax.vmap\n",
        "def binary_operator(q_i, q_j):\n",
        "    A_i, b_i = q_i\n",
        "    A_j, b_j = q_j\n",
        "    return A_j * A_i, A_j * b_i + b_j\n",
        "\n",
        "def parallel_scan(A, B, C, inputs):\n",
        "    A_elements = A * jnp.ones((inputs.shape[0], A.shape[0]))\n",
        "    Bu_elements = jax.vmap(lambda u: B @ u)(inputs)\n",
        "    # Jax's built-in associative scan really comes in handy here as it executes a similar scan\n",
        "    # operation as used in a normal recurrent unroll but is specifically tailored to fit an associative\n",
        "    # operation like the one described in the paper.\n",
        "    _, xs = jax.lax.associative_scan(binary_operator, (A_elements, Bu_elements))\n",
        "    return jax.vmap(lambda x: (C @ x).real)(xs)"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n",
          "is_executing": true
        },
        "id": "1gxWu4Yrg6tC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's that simple! In the original S4 we would have had to apply an independent singe-input, single-output (SISO) SSM for each feature of the input sequence such as in this excerpt from Sasha Rush's Flax implementation:"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "3KfEw2x7g6tC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```python\n",
        "def cloneLayer(layer):\n",
        "    return flax.linen.vmap(\n",
        "        layer,\n",
        "        in_axes=1,\n",
        "        out_axes=1,\n",
        "        variable_axes={\"params\": 1, \"cache\": 1, \"prime\": 1},\n",
        "        split_rngs={\"params\": True},\n",
        "    )\n",
        "SSMLayer = cloneLayer(SSMLayer)\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "gqDe8Dxuc33-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Whereas in the S5 we process the entire sequence in one multi-input, multi-output (MIMO) layer."
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "fvQsAYVtg6tD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's now rewrite our Module as a full S5 layer using this new method, we will be adding a few extra conditional arguments as well as changing some parameterization to match the original paper, but we'll walk through the reason for all these changes below."
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "vC7ht3zAg6tD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "outputs": [],
      "source": [
        "# First we add a new helper function for the timescale initialization, this one just takes the previous\n",
        "# log_step_initializer and stores a bunch of them in an array since our model is now multi-in, multi-out.\n",
        "def init_log_steps(shape, dtype):\n",
        "    H = shape[0]\n",
        "    log_steps = []\n",
        "    for i in range(H):\n",
        "        log_step = log_step_initializer()(shape=(1,), dtype=dtype)\n",
        "        log_steps.append(log_step)\n",
        "\n",
        "    return jnp.array(log_steps)\n",
        "\n",
        "# We will also rewrite our discretization for the MIMO context\n",
        "def discretize(A, B, delta_t):\n",
        "    Identity = jnp.ones(A.shape[0])\n",
        "    _A = jnp.exp(A*delta_t)\n",
        "    _B = (1/A * (_A-Identity))[..., None] * B\n",
        "    return _A, _B"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n",
          "is_executing": true
        },
        "id": "fmsha-9Fg6tD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "outputs": [],
      "source": [
        "class S5(hk.Module):\n",
        "    def __init__(self,\n",
        "                 state_size: int,\n",
        "\n",
        "                 # Now that we're MIMO we'll need to know the number of input features, commonly\n",
        "                 # referred to as the dimension of the model.\n",
        "                 d_model: int,\n",
        "\n",
        "                 # We must also now specify the number of blocks that we will split our matrices\n",
        "                 # into due to the MIMO context.\n",
        "                 n_blocks: int,\n",
        "\n",
        "                 # Short for conjugate symmetry, because our state matrix is complex we can half\n",
        "                 # the size of it since complex numbers are a real and imaginary number joined together,\n",
        "                 # this is not new to the S5, we just didn't mention it above.\n",
        "                 conj_sym: bool = True,\n",
        "\n",
        "                 # Another standard SSM argument that we omitted above for simplicity's sake,\n",
        "                 # this forces the real part of the state matrix to be negative for better\n",
        "                 # stability, especially in autoregressive tasks.\n",
        "                 clip_eigns: bool = False,\n",
        "\n",
        "                 # Like most RNNs, the S5 can be run in both directions if need be.\n",
        "                 bidirectional: bool = False,\n",
        "\n",
        "                 # Rescales delta_t for varying input resolutions, such as different audio\n",
        "                 # sampling rates.\n",
        "                 step_rescale: float = 1.0,\n",
        "                 name: Optional[str] = None\n",
        "    ):\n",
        "        super(S5, self).__init__(name=name)\n",
        "        self.conj_sym = conj_sym\n",
        "        self.bidirectional = bidirectional\n",
        "\n",
        "        # Note that the Hippo class takes conj_sym as an argument and will automatically half\n",
        "        # the state size provided in its initialization, which is why we need to provide a local\n",
        "        # state size that matches this for the shape argument in hk.get_parameter().\n",
        "\n",
        "        if conj_sym:\n",
        "            _state_size = state_size // 2\n",
        "        else:\n",
        "            _state_size = state_size\n",
        "\n",
        "        # With block_diagonal set as True and the number of blocks provided, our Hippo class\n",
        "        # will automatically handle this change of structure.\n",
        "\n",
        "        _hippo = Hippo(\n",
        "            state_size=state_size,\n",
        "            basis_measure='legs',\n",
        "            conj_sym=conj_sym,\n",
        "            block_diagonal=True,\n",
        "            n_blocks=n_blocks,\n",
        "        )\n",
        "        _hippo()\n",
        "\n",
        "        self._lambda_real = hk.get_parameter(\n",
        "            'lambda_real',\n",
        "            [_state_size],\n",
        "            init=_hippo.lambda_initializer('real')\n",
        "        )\n",
        "        self._lambda_imag = hk.get_parameter(\n",
        "            'lambda_imaginary',\n",
        "            [_state_size],\n",
        "            init=_hippo.lambda_initializer('imaginary')\n",
        "        )\n",
        "        if clip_eigns:\n",
        "            self._lambda = jnp.clip(self._lambda_real, None, -1e-4) + 1j * self._lambda_imag\n",
        "        else:\n",
        "            self._A = self._lambda_real + 1j * self._lambda_imag\n",
        "\n",
        "        # If you recall, I mentioned above that we are automatically using a diagonalized version of\n",
        "        # the HiPPO state matrix rather than the pure one, due to it being very hard to efficiently\n",
        "        # compute. I will now go into a little more detail on how this diagonal representation is\n",
        "        # derived, as it is important for how we initialize the input and output matrices. The diagonal\n",
        "        # decomposition of our state matrix is based on equivalence relation on the SSM parameters:\n",
        "        # (A, B, C) ∼ (V−1AV ,V−1B, CV) with V being the eigenvector of our original A matrix and V-1\n",
        "        # being the inverse eigenvector. The Hippo class has already performed the decomposition of A\n",
        "        # into (V-1AV) automatically, but we have not yet performed the decomposition of B and C, we will\n",
        "        # use the eigenvector_transform class method for that below, but first we must initialize B and C\n",
        "        # as normal distributions, lecun normal and truncated normal respectively. I will note that there\n",
        "        # are a few other options provided for C in the original repository but, to keep it simple, we will\n",
        "        # just use one here.\n",
        "\n",
        "        b_init = hk.initializers.VarianceScaling()\n",
        "        b_shape = [state_size, d_model]\n",
        "        b_init = b_init(b_shape, dtype=jnp.complex64)\n",
        "        self._B = hk.get_parameter(\n",
        "            'B',\n",
        "            [_state_size, d_model, 2],\n",
        "            init=_hippo.eigenvector_transform(b_init,  concatenate=True),\n",
        "        )\n",
        "        B = self._B[..., 0] + 1j * self._B[..., 1]\n",
        "\n",
        "        c_init = hk.initializers.TruncatedNormal()\n",
        "        c_shape = [d_model, state_size, 2]\n",
        "        c_init = c_init(c_shape, dtype=jnp.complex64)\n",
        "        self._C = hk.get_parameter(\n",
        "            'C',\n",
        "            [d_model, _state_size, 2],\n",
        "            init=_hippo.eigenvector_transform(c_init, inverse=False, concatenate=True),\n",
        "        )\n",
        "        # We need two output heads if bidirectional is True.\n",
        "        if bidirectional:\n",
        "            self._C2 = hk.get_parameter(\n",
        "                'C2',\n",
        "                [d_model, _state_size, 2],\n",
        "                init=_hippo.eigenvector_transform(c_init, inverse=False, concatenate=True),\n",
        "            )\n",
        "            C1 = self._C[..., 0] + 1j * self._C[..., 1]\n",
        "            C2 = self._C2[..., 0] + 1j * self._C2[..., 1]\n",
        "            self._output_matrix = jnp.concatenate((C1, C2), axis=-1)\n",
        "        else:\n",
        "            self._output_matrix = self._C[..., 0] + 1j * self._C[..., 1]\n",
        "\n",
        "        self._D = hk.get_parameter(\n",
        "            'D',\n",
        "            [d_model,],\n",
        "            init=hk.initializers.RandomNormal(stddev=1.0)\n",
        "        )\n",
        "\n",
        "        self._delta_t = hk.get_parameter(\n",
        "            'delta_T',\n",
        "            [_state_size, 1],\n",
        "            init=init_log_steps\n",
        "        )\n",
        "        timescale = step_rescale * jnp.exp(self._delta_t[:, 0])\n",
        "\n",
        "        # We could also use the bilinear discretization method, but we'll just stick to zoh for now.\n",
        "        self._state_matrix, self._input_matrix = discretize(self._A, B, timescale)\n",
        "\n",
        "\n",
        "    def __call__(self, inputs):\n",
        "        # Note that this is the exact same function as presented above just with alternate procedures\n",
        "        # depending on the bidirectional and conjugate symmetry arguments\n",
        "\n",
        "        A_elements = self._state_matrix * jnp.ones((inputs.shape[0], self._state_matrix.shape[0]))\n",
        "        Bu_elements = jax.vmap(lambda u: self._input_matrix @ u)(inputs)\n",
        "\n",
        "        _, xs = jax.lax.associative_scan(binary_operator, (A_elements, Bu_elements))\n",
        "\n",
        "        if self.bidirectional:\n",
        "            _, xs2 = jax.lax.associative_scan(binary_operator,\n",
        "                                              (A_elements, Bu_elements),\n",
        "                                              reverse=True)\n",
        "            xs = jnp.concatenate((xs, xs2), axis=-1)\n",
        "\n",
        "        if self.conj_sym:\n",
        "            ys = jax.vmap(lambda x: 2*(self._output_matrix @ x).real)(xs)\n",
        "        else:\n",
        "            ys = jax.vmap(lambda x: (self._output_matrix @ x).real)(xs)\n",
        "\n",
        "        Du = jax.vmap(lambda u: self._D * u)(inputs)\n",
        "\n",
        "        return ys + Du"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n",
          "is_executing": true
        },
        "id": "pu0NZItog6tD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There we have it, a complete S5 layer! Now let's form a block around it using a structure very similar to a transformer block with a Gated Linear Unit (GLU)."
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "BwSd9kcOg6tD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "outputs": [],
      "source": [
        "import dataclasses\n",
        "\n",
        "@dataclasses.dataclass\n",
        "class S5Block(hk.Module):\n",
        "    ssm: S5\n",
        "    d_model: int\n",
        "    dropout_rate: float\n",
        "    prenorm: bool\n",
        "    istraining: bool = True\n",
        "    name: Optional[str] = None\n",
        "\n",
        "    def __post_init__(self):\n",
        "        super(S5Block, self).__post_init__()\n",
        "        # We could use either layer norm or batch norm.\n",
        "        self._norm = hk.LayerNorm(axis=-1, create_scale=True, create_offset=True)\n",
        "        self._linear = hk.Linear(self.d_model)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        skip = x\n",
        "        if self.prenorm:\n",
        "            x = self._norm(x)\n",
        "\n",
        "        x = self.ssm(x)\n",
        "        # There are a couple of other GLU patterns we could use here, but once again I have chosen\n",
        "        # one semi-arbitrarily to avoid cluttering our module with if statements.\n",
        "        x1 = hk.dropout(hk.next_rng_key(), self.dropout_rate, jax.nn.gelu(x))\n",
        "        x = x * jax.nn.sigmoid(self._linear(x1))\n",
        "        x = hk.dropout(hk.next_rng_key(), self.dropout_rate, x)\n",
        "\n",
        "        x = skip + x\n",
        "        if not self.prenorm:\n",
        "            x = self._norm(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n",
          "is_executing": true
        },
        "id": "p9oMO9Tfg6tD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's make a stack of these blocks:"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "nxrqTJJXg6tE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "outputs": [],
      "source": [
        "@dataclasses.dataclass\n",
        "class S5Stack(hk.Module):\n",
        "    ssm: S5\n",
        "    d_model: int\n",
        "    n_layers: int\n",
        "    dropout_rate: float\n",
        "    prenorm: bool\n",
        "    istraining: bool = True\n",
        "    name: Optional[str] = None\n",
        "\n",
        "    def __post_init__(self):\n",
        "        super(S5Stack, self).__post_init__(name=self.name)\n",
        "        self._encoder = hk.Linear(self.d_model)\n",
        "        self._layers = [\n",
        "            S5Block(\n",
        "                ssm=self.ssm,\n",
        "                d_model=self.d_model,\n",
        "                dropout_rate=self.dropout_rate,\n",
        "                istraining=self.istraining,\n",
        "                prenorm=self.prenorm,\n",
        "            )\n",
        "            for _ in range(self.n_layers)\n",
        "        ]\n",
        "\n",
        "    def __call__(self, x):\n",
        "        x = self._encoder(x)\n",
        "        for layer in self._layers:\n",
        "            x = layer(x)\n",
        "        return x"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n",
          "is_executing": true
        },
        "id": "Sa0yRsNsg6tE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementing Regression in S5"
      ],
      "metadata": {
        "id": "uAD9a1Oyr2Hr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a new dataset class to hold our stock data"
      ],
      "metadata": {
        "id": "zFLCFHwusNTm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- New Data Section ---\n",
        "# This custom Dataset class will hold our sliding windows\n",
        "class StockWindowDataset(TorchDataset):\n",
        "    def __init__(self, data, labels, last_prices):\n",
        "        # Explicitly convert JAX array -> NumPy array -> Torch tensor\n",
        "        self.data = torch.tensor(np.array(data), dtype=torch.float32)\n",
        "        self.labels = torch.tensor(np.array(labels), dtype=torch.float32)\n",
        "        # Store the last price of the input window (unscaled)\n",
        "        self.last_prices = torch.tensor(np.array(last_prices), dtype=torch.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # We need to add an extra dimension for the input feature\n",
        "        inputs = self.data[idx][..., None]\n",
        "        last_price = self.last_prices[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # Return inputs as a tuple (sequence, last_price_anchor)\n",
        "        return (inputs, last_price), label\n",
        "\n",
        "def create_stock_dataset(batch_size=64, in_seq_length=7, out_seq_length=7):\n",
        "    print(\"[*] Generating Stock Price Dataset (Log Returns)...\")\n",
        "\n",
        "    # 1. Download data\n",
        "    ticker = \"^GSPC\"\n",
        "    data = yf.download(ticker, start=\"2000-01-01\", end=\"2023-12-31\")\n",
        "\n",
        "    # We'll use 'Close' as the representative \"average\" price\n",
        "    prices_flat = data[['Close']].values.flatten()\n",
        "\n",
        "    # 2. Calculate Log Returns\n",
        "    # This will result in (N-1) data points\n",
        "    log_returns = np.log(prices_flat[1:] / prices_flat[:-1])\n",
        "    # The 'anchor' prices are the prices *before* the log return\n",
        "    # This also has (N-1) data points, aligned with log_returns\n",
        "    anchor_prices = prices_flat[:-1]\n",
        "\n",
        "    # 3. Split into train and test sets (BEFORE scaling)\n",
        "    train_split_idx = int(len(log_returns) * 0.8)\n",
        "\n",
        "    lr_train = log_returns[:train_split_idx]\n",
        "    lr_test = log_returns[train_split_idx:]\n",
        "\n",
        "    prices_train = anchor_prices[:train_split_idx]\n",
        "    prices_test = anchor_prices[train_split_idx:]\n",
        "\n",
        "    # 4. Normalise the log returns\n",
        "    # We fit the scaler ONLY on the training log returns to prevent data leakage\n",
        "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "\n",
        "    # Fit on training log returns\n",
        "    lr_train_scaled = scaler.fit_transform(lr_train.reshape(-1, 1)).flatten()\n",
        "\n",
        "    # Transform test log returns\n",
        "    lr_test_scaled = scaler.transform(lr_test.reshape(-1, 1)).flatten()\n",
        "\n",
        "    # 5. Create sliding windows\n",
        "    # Helper function to create windows\n",
        "    def _create_windows(scaled_data, price_data, in_len, out_len):\n",
        "        X_lr, y_lr, last_prices = [], [], []\n",
        "        # We need to stop in_len + out_len from the end\n",
        "        for i in range(len(scaled_data) - in_len - out_len + 1):\n",
        "            X_lr.append(scaled_data[i : i + in_len])\n",
        "            y_lr.append(scaled_data[i + in_len : i + in_len + out_len])\n",
        "            # Get the last *actual price* of the input window\n",
        "            last_prices.append(price_data[i + in_len - 1])\n",
        "\n",
        "        return jnp.array(X_lr), jnp.array(y_lr), jnp.array(last_prices)\n",
        "\n",
        "    X_train, y_train, last_prices_train = _create_windows(\n",
        "        lr_train_scaled, prices_train, in_seq_length, out_seq_length\n",
        "    )\n",
        "    X_test, y_test, last_prices_test = _create_windows(\n",
        "        lr_test_scaled, prices_test, in_seq_length, out_seq_length\n",
        "    )\n",
        "\n",
        "    print(f\"[*] Train samples: {len(X_train)}, Test samples: {len(X_test)}\")\n",
        "\n",
        "    # 6. Create DataLoaders\n",
        "    train_dataset = StockWindowDataset(X_train, y_train, last_prices_train)\n",
        "    test_dataset = StockWindowDataset(X_test, y_test, last_prices_test)\n",
        "\n",
        "    trainloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    testloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # N_CLASSES is now the output dimension (7)\n",
        "    OUT_DIM = out_seq_length # This will be 7\n",
        "    IN_DIM = 1 # Input is 1D (log return)\n",
        "\n",
        "    # We also return the scaler so we can invert the predictions later\n",
        "    return trainloader, testloader, OUT_DIM, in_seq_length, IN_DIM, scaler\n",
        "\n",
        "# Replace the Datasets dict and create_dataset function\n",
        "Datasets = {\n",
        "    \"stock-prediction\": create_stock_dataset,\n",
        "}\n",
        "\n",
        "# Simple class for storing our dataset parameters\n",
        "class Dataset(NamedTuple):\n",
        "    trainloader: DataLoader\n",
        "    testloader: DataLoader\n",
        "    n_classes: int      # This will be OUT_SEQ_LENGTH\n",
        "    in_seq_length: int\n",
        "    out_seq_length: int\n",
        "    d_input: int        # This will be 1\n",
        "    scaler: MinMaxScaler # Store the scaler\n",
        "    classification: bool\n",
        "\n",
        "def create_dataset(dataset: str, batch_size: int, in_seq_length: int, out_seq_length: int) -> Dataset:\n",
        "    classification = 'classification' in dataset\n",
        "    dataset_init = Datasets[dataset]\n",
        "\n",
        "    # Pass seq_length to the function\n",
        "    # Note: the `in_seq_length` returned here is the same one we passed in\n",
        "    trainloader, testloader, OUT_DIM, _, IN_DIM, scaler = dataset_init(\n",
        "        batch_size=batch_size, in_seq_length=in_seq_length, out_seq_length = out_seq_length\n",
        "    )\n",
        "\n",
        "    return Dataset(\n",
        "        trainloader,        # 1. trainloader\n",
        "        testloader,         # 2. testloader\n",
        "        OUT_DIM,            # 3. n_classes\n",
        "        in_seq_length,      # 4. in_seq_length\n",
        "        out_seq_length,     # 5. out_seq_length\n",
        "        IN_DIM,             # 6. d_input\n",
        "        scaler,             # 7. scaler\n",
        "        classification      # 8. classification\n",
        "    )"
      ],
      "metadata": {
        "id": "5dl5NtHfr7Cv"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating our Regressor"
      ],
      "metadata": {
        "id": "UzZvMnv4whWZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from re import X\n",
        "\n",
        "# Replace S5Classifier with S5Regressor\n",
        "@dataclasses.dataclass\n",
        "class S5Regressor(hk.Module):\n",
        "    ssm: S5\n",
        "    d_model: int\n",
        "    d_output: int      # This will be OUT_SEQ_LENGTH\n",
        "    n_layers: int\n",
        "    dropout_rate: float\n",
        "    mode: str = 'last' # 'last' is more common for time-series prediction\n",
        "    prenorm: bool = True\n",
        "    istraining: bool = True\n",
        "    name: Optional[str] = None\n",
        "\n",
        "    def __post_init__(self):\n",
        "        super(S5Regressor, self).__post_init__(name=self.name)\n",
        "        self._encoder = S5Stack(\n",
        "            ssm=self.ssm,\n",
        "            d_model=self.d_model,\n",
        "            n_layers=self.n_layers,\n",
        "            dropout_rate=self.dropout_rate,\n",
        "            istraining=self.istraining,\n",
        "            prenorm=self.prenorm,\n",
        "        )\n",
        "        self._decoder = hk.Linear(self.d_output)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        x = self._encoder(x)\n",
        "\n",
        "        # For predicting the next step, using the 'last' hidden state\n",
        "        # is usually the best approach.\n",
        "        if self.mode == 'pool':\n",
        "            x = jnp.mean(x, axis=0)\n",
        "        elif self.mode == 'last':\n",
        "            x = x[-1] # Take the last element of the sequence\n",
        "        else:\n",
        "            raise NotImplementedError(\"Mode must be in ['pool', 'last]\")\n",
        "\n",
        "        x = self._decoder(x)\n",
        "\n",
        "        # CRITICAL: Remove the log_softmax! We want the raw output value.\n",
        "        return x"
      ],
      "metadata": {
        "id": "I-5OkcXLspHI"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we will set some hyperparameters:"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "Oen4DWzdg6tE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "outputs": [],
      "source": [
        "# --- Set Hyperparameters ---\n",
        "# These are a good start, but you will need to tune them for this new task.\n",
        "STATE_SIZE: int = 64    # Reduced for a simpler 1D input\n",
        "D_MODEL: int = 32     # Reduced\n",
        "N_LAYERS: int = 4\n",
        "N_BLOCKS: int = 4\n",
        "EPOCHS: int = 50\n",
        "BATCH_SIZE: int = 64\n",
        "DROPOUT_RATE: float = 0.1\n",
        "LEARNING_RATE: float = 1e-3\n",
        "SEED = 0\n",
        "IN_SEQ_LENGTH: int = 252  # One trading year of history\n",
        "OUT_SEQ_LENGTH: int = 7   # Predict next 7 days"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n",
          "is_executing": true
        },
        "id": "5RsrP52qg6tE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To keep things simple for this example, we will just be using a plain adam optimizer, but the results can be highly improved with extra techniques such as cosine annealing, learning rate schedules and weight decay."
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "URrXKlSyg6tE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "outputs": [],
      "source": [
        "import optax\n",
        "\n",
        "class TrainingState(NamedTuple):\n",
        "    params: hk.Params\n",
        "    opt_state: optax.OptState\n",
        "    rng_key: jnp.ndarray\n",
        "\n",
        "optim = optax.adam(LEARNING_RATE)"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n",
          "is_executing": true
        },
        "id": "838zDBvUg6tE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Now for the loss and update functions:"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "Egd1GpRPg6tF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "outputs": [],
      "source": [
        "from functools import partial\n",
        "from typing import Tuple, MutableMapping, Any\n",
        "_Metrics = MutableMapping[str, Any]\n",
        "\n",
        "# Replace cross_entropy_loss and compute_accuracy\n",
        "@partial(jnp.vectorize, signature=\"(),()->()\")\n",
        "def mse_loss(prediction, target) -> jnp.ndarray:\n",
        "    \"\"\"Computes Mean Squared Error loss.\"\"\"\n",
        "    return (prediction - target) ** 2\n",
        "\n",
        "# We no longer need compute_accuracy\n",
        "\n",
        "# --- Modify the 'update' function ---\n",
        "@partial(jax.jit, static_argnums=(3, 4))\n",
        "def update(\n",
        "        state: TrainingState,\n",
        "        inputs: jnp.ndarray,\n",
        "        targets: jnp.ndarray,\n",
        "        model: hk.transform,\n",
        "        classification: bool,) -> Tuple[TrainingState, _Metrics]:\n",
        "\n",
        "    rng_key, next_rng_key = jax.random.split(state.rng_key)\n",
        "\n",
        "    def loss_fn(params):\n",
        "        # 'predictions' instead of 'logits'\n",
        "        predictions = model.apply(params, rng_key, inputs)\n",
        "\n",
        "        # Use MSE loss (on scaled log returns)\n",
        "        _loss = jnp.mean(mse_loss(predictions, targets))\n",
        "\n",
        "        # We return loss as the primary value and as an aux value\n",
        "        return _loss, _loss\n",
        "\n",
        "    grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
        "\n",
        "    # (loss, loss_metric), gradients\n",
        "    (loss, _), gradients = grad_fn(state.params)\n",
        "\n",
        "    updates, new_opt_state = optim.update(gradients, state.opt_state, state.params)\n",
        "    new_params = optax.apply_updates(state.params, updates)\n",
        "\n",
        "    new_state = TrainingState(\n",
        "        params=new_params,\n",
        "        opt_state=new_opt_state,\n",
        "        rng_key=next_rng_key,\n",
        "    )\n",
        "    metrics = {\n",
        "        'loss': loss, # We only track loss now\n",
        "    }\n",
        "\n",
        "    return new_state, metrics\n",
        "\n",
        "# --- Modify the 'evaluate' function ---\n",
        "@partial(jax.jit, static_argnums=(3, 4))\n",
        "def evaluate(\n",
        "        state: TrainingState,\n",
        "        inputs: jnp.ndarray,\n",
        "        targets: jnp.ndarray,\n",
        "        model: hk.transform,\n",
        "        classification,) -> _Metrics:\n",
        "\n",
        "    rng_key, _ = jax.random.split(state.rng_key, 2)\n",
        "\n",
        "    predictions = model.apply(state.params, rng_key, inputs)\n",
        "    loss = jnp.mean(mse_loss(predictions, targets)) # MSE on scaled log returns\n",
        "\n",
        "    metrics = {\n",
        "        'loss': loss, # We only track loss\n",
        "    }\n",
        "\n",
        "    return metrics"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n",
          "is_executing": true
        },
        "id": "VHh5Id9fg6tF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Now we call these update and evaluate functions in their respective epochs:"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "2nCPmxiWg6tF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "# --- Modify 'training_epoch' ---\n",
        "def training_epoch(\n",
        "        state: TrainingState,\n",
        "        trainloader: DataLoader,\n",
        "        model: hk.transform,\n",
        "        classification: bool = False,) -> Tuple[TrainingState, jnp.ndarray]: # Removed accuracy output\n",
        "\n",
        "    batch_losses = [] # Removed batch_accuracies\n",
        "    for batch_idx, (inputs_tuple, targets) in enumerate(tqdm(trainloader)):\n",
        "        # inputs_tuple is (input_sequences, last_prices)\n",
        "        # We only need the input_sequences for training\n",
        "        inputs = jnp.array(inputs_tuple[0].numpy())\n",
        "        targets = jnp.array(targets.numpy())\n",
        "\n",
        "        state, metrics = update(\n",
        "            state, inputs, targets,\n",
        "            model, classification\n",
        "        )\n",
        "        batch_losses.append(metrics['loss'])\n",
        "        # Removed accuracy\n",
        "\n",
        "    return (\n",
        "        state,\n",
        "        jnp.mean(jnp.array(batch_losses)),\n",
        "        # Removed accuracy\n",
        "    )\n",
        "\n",
        "# --- Modify 'validation_epoch' ---\n",
        "def validation_epoch(\n",
        "    state: TrainingState,\n",
        "    testloader: DataLoader,\n",
        "    model: hk.transform,\n",
        "    classification: bool = True,) -> jnp.ndarray: # Removed accuracy output\n",
        "\n",
        "    losses = [] # Removed accuracies\n",
        "    for batch_idx, (inputs_tuple, targets) in enumerate(tqdm(testloader)):\n",
        "        # inputs_tuple is (input_sequences, last_prices)\n",
        "        # We only need the input_sequences for evaluation\n",
        "        inputs = jnp.array(inputs_tuple[0].numpy())\n",
        "        targets = jnp.array(targets.numpy())\n",
        "\n",
        "        metrics = evaluate(\n",
        "            state, inputs, targets,\n",
        "            model, classification\n",
        "        )\n",
        "        losses.append(metrics['loss'])\n",
        "        # Removed accuracy\n",
        "\n",
        "    return jnp.mean(jnp.array(losses)) # Removed accuracy"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n",
          "is_executing": true
        },
        "id": "OXJcCJnvg6tF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[*] Generating Stock Price Dataset (Log Returns)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4238025639.py:28: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  data = yf.download(ticker, start=\"2000-01-01\", end=\"2023-12-31\")\n",
            "[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[*] Train samples: 4570, Test samples: 950\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "# Set random number generators\n",
        "torch.random.manual_seed(SEED)\n",
        "key = jax.random.PRNGKey(SEED)\n",
        "rng, init_rng = jax.random.split(key)\n",
        "\n",
        "# Note: We now pass IN_SEQ_LENGTH AND OUT_SEQ_LENGTH\n",
        "ds = create_dataset(\n",
        "    'stock-prediction',\n",
        "    batch_size = BATCH_SIZE,\n",
        "    in_seq_length = IN_SEQ_LENGTH,\n",
        "    out_seq_length = OUT_SEQ_LENGTH\n",
        ")\n",
        "\n",
        "# Get the first batch and unpack the input sequence\n",
        "init_data_tuple, _ = next(iter(ds.trainloader))\n",
        "init_data = jnp.array(init_data_tuple[0].numpy())"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n",
          "is_executing": true
        },
        "id": "undnZLH7g6tF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f858ed10-a5f4-4efc-efa6-9e2269c46bfb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "outputs": [],
      "source": [
        "# In Haiku, we have to call our model inside a transformed function using hk.transform for it to become\n",
        "# functionally pure and compatible with essential JAX functions like jax.grad(). Here we are using hk.vmap\n",
        "# instead of jax.vmap because we are calling it from within a hk.transform.\n",
        "@hk.transform\n",
        "def forward(x) -> hk.transform:\n",
        "    # Use S5Regressor\n",
        "    neural_net = S5Regressor(\n",
        "        S5(\n",
        "            STATE_SIZE,\n",
        "            D_MODEL,\n",
        "            N_BLOCKS,\n",
        "        ),\n",
        "        D_MODEL,\n",
        "        ds.n_classes,      # This will be OUT_SEQ_LENGTH (7)\n",
        "        N_LAYERS,\n",
        "        DROPOUT_RATE,\n",
        "    )\n",
        "    return hk.vmap(neural_net, split_rng=False)(x)"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n",
          "is_executing": true
        },
        "id": "m_M81cNFg6tF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "outputs": [],
      "source": [
        "# In Haiku, we have to call our model inside a transformed function using hk.transform for it to become\n",
        "# functionally pure and compatible with essential JAX functions like jax.grad(). Here we are using hk.vmap\n",
        "# instead of jax.vmap because we are calling it from within a hk.transform.\n",
        "@hk.transform\n",
        "def forward(x) -> hk.transform:\n",
        "    # Use S5Regressor\n",
        "    neural_net = S5Regressor(\n",
        "        S5(\n",
        "            STATE_SIZE,\n",
        "            D_MODEL,\n",
        "            N_BLOCKS,\n",
        "        ),\n",
        "        D_MODEL,\n",
        "        ds.n_classes,  # This will be 1\n",
        "        N_LAYERS,\n",
        "        DROPOUT_RATE,\n",
        "    )\n",
        "    return hk.vmap(neural_net, split_rng=False)(x)"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n",
          "is_executing": true
        },
        "id": "urrTfYtEg6tF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jax/_src/lax/lax.py:1660: ComplexWarning: Casting complex values to real discards the imaginary part\n",
            "  return _convert_element_type(operand, new_dtype, weak_type=False)  # type: ignore[unused-ignore,bad-return-type]\n"
          ]
        }
      ],
      "source": [
        "# Set state\n",
        "initial_params = forward.init(init_rng, init_data)\n",
        "initial_opt_state = optim.init(initial_params)\n",
        "state = TrainingState(\n",
        "    params=initial_params,\n",
        "    opt_state=initial_opt_state,\n",
        "    rng_key=rng\n",
        ")"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n",
          "is_executing": true
        },
        "id": "-daPDGdlg6tF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4e31c61-fcf1-4e58-956a-639cd4aa51d5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training and Plotting"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "7yFEQtQvg6tG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[*] Training Epoch 1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/72 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/jax/_src/lax/lax.py:5473: ComplexWarning: Casting complex values to real discards the imaginary part\n",
            "  x_bar = _convert_element_type(x_bar, x.aval.dtype, x.aval.weak_type)\n",
            "100%|██████████| 72/72 [01:11<00:00,  1.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[*] Running Epoch 1 Validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 15/15 [00:11<00:00,  1.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=>> Epoch 1 Metrics ===\n",
            "\tTrain Loss (MSE on log returns): 14.41262\n",
            "\t Test Loss (MSE on log returns): 7.03317\n",
            "[*] Training Epoch 2...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 72/72 [00:44<00:00,  1.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[*] Running Epoch 2 Validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 15/15 [00:00<00:00, 26.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=>> Epoch 2 Metrics ===\n",
            "\tTrain Loss (MSE on log returns): 6.34174\n",
            "\t Test Loss (MSE on log returns): 2.14108\n",
            "[*] Training Epoch 3...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 72/72 [00:37<00:00,  1.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[*] Running Epoch 3 Validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 15/15 [00:00<00:00, 16.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=>> Epoch 3 Metrics ===\n",
            "\tTrain Loss (MSE on log returns): 2.96500\n",
            "\t Test Loss (MSE on log returns): 2.94166\n",
            "[*] Training Epoch 4...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 72/72 [00:38<00:00,  1.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[*] Running Epoch 4 Validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 15/15 [00:00<00:00, 26.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=>> Epoch 4 Metrics ===\n",
            "\tTrain Loss (MSE on log returns): 1.95103\n",
            "\t Test Loss (MSE on log returns): 1.51877\n",
            "[*] Training Epoch 5...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 72/72 [00:47<00:00,  1.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[*] Running Epoch 5 Validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 15/15 [00:00<00:00, 27.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=>> Epoch 5 Metrics ===\n",
            "\tTrain Loss (MSE on log returns): 1.47884\n",
            "\t Test Loss (MSE on log returns): 1.98915\n",
            "[*] Training Epoch 6...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 72/72 [00:38<00:00,  1.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[*] Running Epoch 6 Validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 15/15 [00:00<00:00, 26.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=>> Epoch 6 Metrics ===\n",
            "\tTrain Loss (MSE on log returns): 1.16503\n",
            "\t Test Loss (MSE on log returns): 2.74476\n",
            "[*] Training Epoch 7...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 72/72 [00:38<00:00,  1.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[*] Running Epoch 7 Validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 15/15 [00:00<00:00, 26.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=>> Epoch 7 Metrics ===\n",
            "\tTrain Loss (MSE on log returns): 0.99551\n",
            "\t Test Loss (MSE on log returns): 2.71436\n",
            "[*] Training Epoch 8...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 72/72 [00:38<00:00,  1.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[*] Running Epoch 8 Validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 15/15 [00:00<00:00, 23.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=>> Epoch 8 Metrics ===\n",
            "\tTrain Loss (MSE on log returns): 0.91881\n",
            "\t Test Loss (MSE on log returns): 0.38359\n",
            "[*] Training Epoch 9...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 72/72 [00:38<00:00,  1.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[*] Running Epoch 9 Validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 15/15 [00:00<00:00, 26.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=>> Epoch 9 Metrics ===\n",
            "\tTrain Loss (MSE on log returns): 0.65384\n",
            "\t Test Loss (MSE on log returns): 0.32577\n",
            "[*] Training Epoch 10...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 72/72 [00:38<00:00,  1.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[*] Running Epoch 10 Validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 15/15 [00:00<00:00, 26.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=>> Epoch 10 Metrics ===\n",
            "\tTrain Loss (MSE on log returns): 0.57159\n",
            "\t Test Loss (MSE on log returns): 0.43079\n",
            "[*] Training Epoch 11...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 72/72 [00:39<00:00,  1.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[*] Running Epoch 11 Validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 15/15 [00:00<00:00, 26.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=>> Epoch 11 Metrics ===\n",
            "\tTrain Loss (MSE on log returns): 0.42147\n",
            "\t Test Loss (MSE on log returns): 0.32802\n",
            "[*] Training Epoch 12...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 72/72 [00:39<00:00,  1.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[*] Running Epoch 12 Validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 15/15 [00:00<00:00, 26.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=>> Epoch 12 Metrics ===\n",
            "\tTrain Loss (MSE on log returns): 0.31643\n",
            "\t Test Loss (MSE on log returns): 0.66437\n",
            "[*] Training Epoch 13...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 72/72 [00:38<00:00,  1.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[*] Running Epoch 13 Validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 15/15 [00:00<00:00, 26.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=>> Epoch 13 Metrics ===\n",
            "\tTrain Loss (MSE on log returns): 0.29682\n",
            "\t Test Loss (MSE on log returns): 0.26896\n",
            "[*] Training Epoch 14...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 72/72 [00:39<00:00,  1.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[*] Running Epoch 14 Validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 15/15 [00:00<00:00, 26.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=>> Epoch 14 Metrics ===\n",
            "\tTrain Loss (MSE on log returns): 0.28412\n",
            "\t Test Loss (MSE on log returns): 0.66917\n",
            "[*] Training Epoch 15...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 72/72 [00:38<00:00,  1.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[*] Running Epoch 15 Validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 15/15 [00:00<00:00, 17.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=>> Epoch 15 Metrics ===\n",
            "\tTrain Loss (MSE on log returns): 0.26999\n",
            "\t Test Loss (MSE on log returns): 0.31784\n",
            "[*] Training Epoch 16...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 72/72 [00:38<00:00,  1.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[*] Running Epoch 16 Validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 15/15 [00:00<00:00, 27.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=>> Epoch 16 Metrics ===\n",
            "\tTrain Loss (MSE on log returns): 0.21247\n",
            "\t Test Loss (MSE on log returns): 0.66212\n",
            "[*] Training Epoch 17...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 72/72 [00:43<00:00,  1.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[*] Running Epoch 17 Validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 15/15 [00:00<00:00, 25.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=>> Epoch 17 Metrics ===\n",
            "\tTrain Loss (MSE on log returns): 0.20370\n",
            "\t Test Loss (MSE on log returns): 0.15453\n",
            "[*] Training Epoch 18...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 72/72 [00:40<00:00,  1.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[*] Running Epoch 18 Validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 15/15 [00:00<00:00, 27.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=>> Epoch 18 Metrics ===\n",
            "\tTrain Loss (MSE on log returns): 0.18400\n",
            "\t Test Loss (MSE on log returns): 0.18415\n",
            "[*] Training Epoch 19...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 21%|██        | 15/72 [00:02<00:12,  4.63it/s]"
          ]
        }
      ],
      "source": [
        "# --- Lists to store loss history ---\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    print(f\"[*] Training Epoch {epoch + 1}...\")\n",
        "    state, training_loss = training_epoch(\n",
        "        state,\n",
        "        ds.trainloader,\n",
        "        forward,\n",
        "        ds.classification\n",
        "    )\n",
        "\n",
        "    print(f\"[*] Running Epoch {epoch + 1} Validation...\")\n",
        "    test_loss = validation_epoch(\n",
        "        state,\n",
        "        ds.testloader,\n",
        "        forward,\n",
        "        ds.classification\n",
        "    )\n",
        "\n",
        "    # --- Store the losses ---\n",
        "    train_losses.append(training_loss)\n",
        "    test_losses.append(test_loss)\n",
        "\n",
        "    print(f\"\\n=>> Epoch {epoch + 1} Metrics ===\")\n",
        "    print(\n",
        "        f\"\\tTrain Loss (MSE on log returns): {training_loss:.5f}\\n\\t Test Loss (MSE on log returns): {test_loss:.5f}\"\n",
        "    )"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "LEWQ9Rbig6tG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25de8cf6-43fb-4429-a336-e1873d64c9e4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Plot 1: Training & Validation Loss ---\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.plot(train_losses, label='Training Loss')\n",
        "plt.plot(test_losses, label='Test (Validation) Loss')\n",
        "plt.title('Training & Validation Loss Over Epochs (MSE on Scaled Log Returns)')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss (MSE)')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.savefig('loss_plot_log_returns.png')\n",
        "print(\"\\n[*] Loss plot saved to loss_plot_log_returns.png\")\n",
        "\n",
        "\n",
        "# --- Helper function to convert log returns back to prices ---\n",
        "@jax.jit\n",
        "def convert_lr_to_prices(log_returns, anchor_prices):\n",
        "    \"\"\"\n",
        "    Converts log returns back to prices using an anchor price.\n",
        "    log_returns: array of shape [batch_size, seq_len]\n",
        "    anchor_prices: array of shape [batch_size, 1]\n",
        "    \"\"\"\n",
        "    # p_i = p_{i-1} * exp(lr_i)\n",
        "    # Convert log returns to simple returns\n",
        "    returns = jnp.exp(log_returns)\n",
        "    # Cumulatively multiply returns and scale by the anchor price\n",
        "    cumulative_returns = jnp.cumprod(returns, axis=1)\n",
        "    prices = anchor_prices * cumulative_returns\n",
        "    return prices\n",
        "\n",
        "\n",
        "# --- Plot 2: Actual vs. Predicted Prices ---\n",
        "# 1. Get all predictions from the testloader\n",
        "all_predictions_lr_scaled = []\n",
        "all_actuals_lr_scaled = []\n",
        "all_anchor_prices = []\n",
        "\n",
        "testloader_for_plotting = DataLoader(ds.testloader.dataset, batch_size=64, shuffle=False)\n",
        "eval_rng = jax.random.PRNGKey(SEED + 1)\n",
        "\n",
        "print(\"[*] Generating predictions for plotting...\")\n",
        "for inputs_tuple, targets in tqdm(testloader_for_plotting):\n",
        "    inputs_jnp = jnp.array(inputs_tuple[0].numpy())     # Input sequences\n",
        "    anchors_jnp = jnp.array(inputs_tuple[1].numpy())    # Anchor prices\n",
        "    targets_jnp = jnp.array(targets.numpy())            # Target log returns\n",
        "\n",
        "    preds_lr_scaled = forward.apply(state.params, eval_rng, inputs_jnp) # Shape [batch_size, 7]\n",
        "\n",
        "    all_predictions_lr_scaled.append(preds_lr_scaled)\n",
        "    all_actuals_lr_scaled.append(targets_jnp)\n",
        "    all_anchor_prices.append(anchors_jnp)\n",
        "\n",
        "# Concatenate all batches\n",
        "all_predictions_lr_scaled = jnp.concatenate(all_predictions_lr_scaled, axis=0)\n",
        "all_actuals_lr_scaled = jnp.concatenate(all_actuals_lr_scaled, axis=0)\n",
        "all_anchor_prices = jnp.concatenate(all_anchor_prices, axis=0)\n",
        "\n",
        "# 2. Inverse-transform the LOG RETURNS\n",
        "preds_lr_unscaled = ds.scaler.inverse_transform(all_predictions_lr_scaled)\n",
        "actuals_lr_unscaled = ds.scaler.inverse_transform(all_actuals_lr_scaled)\n",
        "\n",
        "# 3. Convert Log Returns to Prices\n",
        "# Reshape anchor prices to [num_samples, 1] for broadcasting\n",
        "all_anchor_prices_expanded = all_anchor_prices[:, None]\n",
        "\n",
        "actual_prices = convert_lr_to_prices(actuals_lr_unscaled, all_anchor_prices_expanded)\n",
        "predicted_prices = convert_lr_to_prices(preds_lr_unscaled, all_anchor_prices_expanded)\n",
        "\n",
        "# 4. Flatten for plotting\n",
        "actual_prices_flat = actual_prices.flatten()\n",
        "predicted_prices_flat = predicted_prices.flatten()\n",
        "\n",
        "# 5. Create the plot\n",
        "plt.figure(figsize=(15, 7))\n",
        "plt.plot(actual_prices_flat, label='Actual Prices', color='blue', alpha=0.7, linewidth=1.0)\n",
        "# Plot predictions with transparency to see overlap\n",
        "plt.plot(predicted_prices_flat, label='Predicted Prices', color='red', alpha=0.5, linewidth=1.0)\n",
        "plt.title('Stock Price Prediction vs. Actual (Test Set) - Converted from Log Returns')\n",
        "plt.xlabel('Time (Test Samples - 7 day chunks)')\n",
        "plt.ylabel('Stock Price (USD)')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.savefig('prediction_plot_log_returns.png')\n",
        "print(\"[*] Prediction plot saved to prediction_plot_log_returns.png\")"
      ],
      "metadata": {
        "id": "V2VAzZSRyOCi"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}