{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sat-A/s5-jax/blob/main/s5-tokenised.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# S5-Tokenised"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "1YMG3-5Hg6s_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The goal of this exercise is to adapt an annotated implementation of S5 https://github.com/JPGoodale/annotated-s5 that was made for classification tasks to regression tasks."
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "dXRZLfGeg6tA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Runtime Setup\n",
        "Ensure runtime is set to GPU to ensure gpu parallelisation speedup"
      ],
      "metadata": {
        "id": "WIpj_s_ZcaAJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dm-haiku\n",
        "!pip install hippox"
      ],
      "metadata": {
        "id": "XylzIGnbooEy",
        "outputId": "60be1781-e540-4715-9480-e3b08694f5d1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: dm-haiku in /usr/local/lib/python3.12/dist-packages (0.0.15)\n",
            "Requirement already satisfied: absl-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from dm-haiku) (1.4.0)\n",
            "Requirement already satisfied: jmp>=0.0.2 in /usr/local/lib/python3.12/dist-packages (from dm-haiku) (0.0.4)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.12/dist-packages (from dm-haiku) (2.0.2)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.12/dist-packages (from dm-haiku) (0.9.0)\n",
            "Requirement already satisfied: hippox in /usr/local/lib/python3.12/dist-packages (0.0.9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "outputs": [],
      "source": [
        "# Core JAX, Haiku, and Optax\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import haiku as hk\n",
        "import optax\n",
        "from functools import partial\n",
        "from typing import NamedTuple, Optional, Tuple, MutableMapping, Any\n",
        "\n",
        "# Imports from your original notebook\n",
        "import torch\n",
        "from hippox.main import Hippo\n",
        "from tqdm import tqdm\n",
        "import dataclasses\n",
        "import numpy asnp # Using numpy for data generation"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n",
          "is_executing": true
        },
        "id": "YYWAK_LIg6tB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First we'll define some helper functions for discretization and timescale initialization as the SSM equation is naturally continuous and must be made discrete to be unrolled as a linear recurrence like standard RNNs."
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "psmzqs8Xg6tB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "outputs": [],
      "source": [
        "# Here we are just using the zero order hold method for its sheer simplicity, with A, B and delta_t denoting the\n",
        "# state matrix, input matrix and change in timescale respectively.\n",
        "\n",
        "def discretize(A, B, delta_t):\n",
        "    Identity = jnp.ones(A.shape[0])\n",
        "    _A = jnp.exp(A*delta_t)\n",
        "    _B = (1/A * (_A-Identity)) * B\n",
        "    return _A, _B\n",
        "\n",
        "# This is a function used to initialize the trainable timescale parameter.\n",
        "def log_step_initializer(dt_min=0.001, dt_max=0.1):\n",
        "    def init(shape, dtype):\n",
        "        uniform = hk.initializers.RandomUniform()\n",
        "        return uniform(shape, dtype)*(jnp.log(dt_max) - jnp.log(dt_min)) + jnp.log(dt_min)\n",
        "    return init\n",
        "\n",
        "# Taken directly from https://github.com/deepmind/dm-haiku/blob/main/haiku/_src/recurrent.py\n",
        "def add_batch(nest, batch_size: Optional[int]):\n",
        "    broadcast = lambda x: jnp.broadcast_to(x, (batch_size,) + x.shape)\n",
        "    return jax.tree_util.tree_map(broadcast, nest)"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n",
          "is_executing": true
        },
        "id": "Ous4HGubg6tB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The linear SSM equation is as follows:\n",
        "$$ x_0(t) = Ax(t) + Bu(t) $$\n",
        "$$ y(t) = Cx(t) + Du(t) $$\n",
        "\n",
        " We will now implement it as a recurrent Haiku module:"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "SaJ9vnolg6tB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "outputs": [],
      "source": [
        "class LinearSSM(hk.RNNCore):\n",
        "    def __init__(self, state_size: int, name: Optional[str] = None):\n",
        "        super(LinearSSM, self).__init__(name=name)\n",
        "        # We won't get into the basis measure families here, just note that they are basically just the\n",
        "        # type of orthogonal polynomial we initialize with, the scaled Legendre measure (LegS) introduced\n",
        "        # in the original HiPPO paper is pretty much the standard initialization and is what is used in the\n",
        "        # main experiments in the S5 paper. I will also note that the Hippo class uses the diagonal representation\n",
        "        # of the state matrix by default, as this has become the standard in neural SSMs since shown to be\n",
        "        # equally effective as the diagonal plus low rank representation in https://arxiv.org/abs/2203.14343\n",
        "        # and then formalized in https://arxiv.org/abs/2206.11893.\n",
        "\n",
        "        _hippo = Hippo(state_size=state_size, basis_measure='legs')\n",
        "        # Must be called for parameters to be initialized\n",
        "        _hippo()\n",
        "\n",
        "        # We register the real and imaginary components of the state matrix A as separate parameters because\n",
        "        # they will have separate gradients in training, they will be conjoined back together and then discretized\n",
        "        # but this will simply be backpropagated through as a transformation of the lambda real and imaginary\n",
        "        # parameters (lambda is just what we call the diagonalized state matrix).\n",
        "\n",
        "        self._lambda_real = hk.get_parameter(\n",
        "            'lambda_real',\n",
        "            shape=[state_size,],\n",
        "            init=_hippo.lambda_initializer('real')\n",
        "        )\n",
        "        self._lambda_imag = hk.get_parameter(\n",
        "            'lambda_imaginary',\n",
        "            shape=[state_size,],\n",
        "            init=_hippo.lambda_initializer('imaginary')\n",
        "        )\n",
        "        self._A = self._lambda_real + 1j * self._lambda_imag\n",
        "\n",
        "       # For now, these initializations of the input and output matrices B and C match the S4D\n",
        "        # parameterization for demonstration purposes, we will implement the S5 versions later.\n",
        "\n",
        "        self._B = hk.get_parameter(\n",
        "            'B',\n",
        "            shape=[state_size,],\n",
        "            init=_hippo.b_initializer()\n",
        "        )\n",
        "        self._C = hk.get_parameter(\n",
        "            'C',\n",
        "            shape=[state_size, 2],\n",
        "            init=hk.initializers.RandomNormal(stddev=0.5**0.5)\n",
        "        )\n",
        "        self._output_matrix = self._C[..., 0] + 1j * self._C[..., 1]\n",
        "\n",
        "        # This feed-through matrix basically acts as a residual connection.\n",
        "        self._D = hk.get_parameter(\n",
        "            'D',\n",
        "            [1,],\n",
        "            init=jnp.ones,\n",
        "        )\n",
        "\n",
        "        self._delta_t = hk.get_parameter(\n",
        "            'delta_t',\n",
        "            shape=[1,],\n",
        "            init=log_step_initializer()\n",
        "        )\n",
        "        timescale = jnp.exp(self._delta_t)\n",
        "\n",
        "        self._state_matrix, self._input_matrix = discretize(self._A, self._B, timescale)\n",
        "\n",
        "    def __call__(self, inputs, prev_state):\n",
        "        u = inputs[:, jnp.newaxis]\n",
        "        new_state = self._state_matrix @ prev_state + self._input_matrix @ u\n",
        "        y_s = self._output_matrix @ new_state\n",
        "        out = y_s.reshape(-1).real + self._D * u\n",
        "        return out, new_state\n",
        "\n",
        "    def initial_state(self, batch_size: Optional[int] = None):\n",
        "        state = jnp.zeros([self._state_size])\n",
        "        if batch_size is not None:\n",
        "            state = add_batch(state, batch_size)\n",
        "        return state"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n",
          "is_executing": true
        },
        "id": "PySpa6EMg6tC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You may notice that this looks an awful lot like a vanilla RNN cell, just with our special parameterization and without any activations, hence being a linear recurrence. I have initialized it as an instance of Haiku's RNN.Core abstract base class so that it can be unrolled using either the hk.dynamic_unroll or hk.static_unroll functions like any other recurrent module, however, if you are familiar with any of the S4 models you may be noticing that there's something crucial missing here: the convolutional representation. One of the key contributions of the S4 paper was its demonstration that the SSM ODE can be represented as either a linear recurrence, as above, for efficient inference, or as a global convolution for much faster training. That paper and the following papers then go on to present various kernels for efficiently computing this convolution with Fast Fourier Transforms, highly improving the computational efficiency of the model. Then why have we omitted them? Because the S5 architecture which we are about to explore simplifies all this by providing a purely recurrent representation in both training and inference, it does this by using a parallel recurrence that actually looks alot like a convolution itself! From the paper:\n",
        "\n",
        "    \"We use parallel scans to efficiently compute the states of a discretized linear SSM. Given a binary associative operator • (i.e. (a • b) • c = a • (b • c)) and a sequence of L elements [a1, a2, ..., aL], the scan operation (sometimes referred to as all-prefix-sum) returns the sequence [a1, (a1 • a2), ..., (a1 • a2 • ... • aL)].\"\n",
        "\n",
        "Let's see what this looks like in code, taken straight from the original author's implementation:"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "S7kVHuCkg6tC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "outputs": [],
      "source": [
        "@jax.vmap\n",
        "def binary_operator(q_i, q_j):\n",
        "    A_i, b_i = q_i\n",
        "    A_j, b_j = q_j\n",
        "    return A_j * A_i, A_j * b_i + b_j\n",
        "\n",
        "def parallel_scan(A, B, C, inputs):\n",
        "    A_elements = A * jnp.ones((inputs.shape[0], A.shape[0]))\n",
        "    Bu_elements = jax.vmap(lambda u: B @ u)(inputs)\n",
        "    # Jax's built-in associative scan really comes in handy here as it executes a similar scan\n",
        "    # operation as used in a normal recurrent unroll but is specifically tailored to fit an associative\n",
        "    # operation like the one described in the paper.\n",
        "    _, xs = jax.lax.associative_scan(binary_operator, (A_elements, Bu_elements))\n",
        "    return jax.vmap(lambda x: (C @ x).real)(xs)"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n",
          "is_executing": true
        },
        "id": "1gxWu4Yrg6tC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's that simple! In the original S4 we would have had to apply an independent singe-input, single-output (SISO) SSM for each feature of the input sequence such as in this excerpt from Sasha Rush's Flax implementation:"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "3KfEw2x7g6tC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```python\n",
        "def cloneLayer(layer):\n",
        "    return flax.linen.vmap(\n",
        "        layer,\n",
        "        in_axes=1,\n",
        "        out_axes=1,\n",
        "        variable_axes={\"params\": 1, \"cache\": 1, \"prime\": 1},\n",
        "        split_rngs={\"params\": True},\n",
        "    )\n",
        "SSMLayer = cloneLayer(SSMLayer)\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "gqDe8Dxuc33-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Whereas in the S5 we process the entire sequence in one multi-input, multi-output (MIMO) layer."
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "fvQsAYVtg6tD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's now rewrite our Module as a full S5 layer using this new method, we will be adding a few extra conditional arguments as well as changing some parameterization to match the original paper, but we'll walk through the reason for all these changes below."
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "vC7ht3zAg6tD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "outputs": [],
      "source": [
        "# First we add a new helper function for the timescale initialization, this one just takes the previous\n",
        "# log_step_initializer and stores a bunch of them in an array since our model is now multi-in, multi-out.\n",
        "\n",
        "def init_log_steps(shape, dtype):\n",
        "    H = shape[0]\n",
        "    log_steps = []\n",
        "    for i in range(H):\n",
        "        log_step = log_step_initializer()(shape=(1,), dtype=dtype)\n",
        "        log_steps.append(log_step)\n",
        "\n",
        "    return jnp.array(log_steps)"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n",
          "is_executing": true
        },
        "id": "fmsha-9Fg6tD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "outputs": [],
      "source": [
        "# We will also rewrite our discretization for the MIMO context\n",
        "def discretize(A, B, delta_t):\n",
        "    Identity = jnp.ones(A.shape[0])\n",
        "    _A = jnp.exp(A*delta_t)\n",
        "    _B = (1/A * (_A-Identity))[..., None] * B\n",
        "    return _A, _B"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n",
          "is_executing": true
        },
        "id": "J6NX3QPCg6tD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "outputs": [],
      "source": [
        "class S5(hk.Module):\n",
        "    def __init__(self,\n",
        "                 state_size: int,\n",
        "\n",
        "                 # Now that we're MIMO we'll need to know the number of input features, commonly\n",
        "                 # referred to as the dimension of the model.\n",
        "                 d_model: int,\n",
        "\n",
        "                 # We must also now specify the number of blocks that we will split our matrices\n",
        "                 # into due to the MIMO context.\n",
        "                 n_blocks: int,\n",
        "\n",
        "                 # Short for conjugate symmetry, because our state matrix is complex we can half\n",
        "                 # the size of it since complex numbers are a real and imaginary number joined together,\n",
        "                 # this is not new to the S5, we just didn't mention it above.\n",
        "                 conj_sym: bool = True,\n",
        "\n",
        "                 # Another standard SSM argument that we omitted above for simplicity's sake,\n",
        "                 # this forces the real part of the state matrix to be negative for better\n",
        "                 # stability, especially in autoregressive tasks.\n",
        "                 clip_eigns: bool = False,\n",
        "\n",
        "                 # Like most RNNs, the S5 can be run in both directions if need be.\n",
        "                 bidirectional: bool = False,\n",
        "\n",
        "                 # Rescales delta_t for varying input resolutions, such as different audio\n",
        "                 # sampling rates.\n",
        "                 step_rescale: float = 1.0,\n",
        "                 name: Optional[str] = None\n",
        "    ):\n",
        "        super(S5, self).__init__(name=name)\n",
        "        self.conj_sym = conj_sym\n",
        "        self.bidirectional = bidirectional\n",
        "\n",
        "        # Note that the Hippo class takes conj_sym as an argument and will automatically half\n",
        "        # the state size provided in its initialization, which is why we need to provide a local\n",
        "        # state size that matches this for the shape argument in hk.get_parameter().\n",
        "\n",
        "        if conj_sym:\n",
        "            _state_size = state_size // 2\n",
        "        else:\n",
        "            _state_size = state_size\n",
        "\n",
        "        # With block_diagonal set as True and the number of blocks provided, our Hippo class\n",
        "        # will automatically handle this change of structure.\n",
        "\n",
        "        _hippo = Hippo(\n",
        "            state_size=state_size,\n",
        "            basis_measure='legs',\n",
        "            conj_sym=conj_sym,\n",
        "            block_diagonal=True,\n",
        "            n_blocks=n_blocks,\n",
        "        )\n",
        "        _hippo()\n",
        "\n",
        "        self._lambda_real = hk.get_parameter(\n",
        "            'lambda_real',\n",
        "            [_state_size],\n",
        "            init=_hippo.lambda_initializer('real')\n",
        "        )\n",
        "        self._lambda_imag = hk.get_parameter(\n",
        "            'lambda_imaginary',\n",
        "            [_state_size],\n",
        "            init=_hippo.lambda_initializer('imaginary')\n",
        "        )\n",
        "        if clip_eigns:\n",
        "            self._lambda = jnp.clip(self._lambda_real, None, -1e-4) + 1j * self._lambda_imag\n",
        "        else:\n",
        "            self._A = self._lambda_real + 1j * self._lambda_imag\n",
        "\n",
        "        # If you recall, I mentioned above that we are automatically using a diagonalized version of\n",
        "        # the HiPPO state matrix rather than the pure one, due to it being very hard to efficiently\n",
        "        # compute. I will now go into a little more detail on how this diagonal representation is\n",
        "        # derived, as it is important for how we initialize the input and output matrices. The diagonal\n",
        "        # decomposition of our state matrix is based on equivalence relation on the SSM parameters:\n",
        "        # (A, B, C) ∼ (V−1AV ,V−1B, CV) with V being the eigenvector of our original A matrix and V-1\n",
        "        # being the inverse eigenvector. The Hippo class has already performed the decomposition of A\n",
        "        # into (V-1AV) automatically, but we have not yet performed the decomposition of B and C, we will\n",
        "        # use the eigenvector_transform class method for that below, but first we must initialize B and C\n",
        "        # as normal distributions, lecun normal and truncated normal respectively. I will note that there\n",
        "        # are a few other options provided for C in the original repository but, to keep it simple, we will\n",
        "        # just use one here.\n",
        "\n",
        "        b_init = hk.initializers.VarianceScaling()\n",
        "        b_shape = [state_size, d_model]\n",
        "        b_init = b_init(b_shape, dtype=jnp.complex64)\n",
        "        self._B = hk.get_parameter(\n",
        "            'B',\n",
        "            [_state_size, d_model, 2],\n",
        "            init=_hippo.eigenvector_transform(b_init,  concatenate=True),\n",
        "        )\n",
        "        B = self._B[..., 0] + 1j * self._B[..., 1]\n",
        "\n",
        "        c_init = hk.initializers.TruncatedNormal()\n",
        "        c_shape = [d_model, state_size, 2]\n",
        "        c_init = c_init(c_shape, dtype=jnp.complex64)\n",
        "        self._C = hk.get_parameter(\n",
        "            'C',\n",
        "            [d_model, _state_size, 2],\n",
        "            init=_hippo.eigenvector_transform(c_init, inverse=False, concatenate=True),\n",
        "        )\n",
        "        # We need two output heads if bidirectional is True.\n",
        "        if bidirectional:\n",
        "            self._C2 = hk.get_parameter(\n",
        "                'C2',\n",
        "                [d_model, _state_size, 2],\n",
        "                init=_hippo.eigenvector_transform(c_init, inverse=False, concatenate=True),\n",
        "            )\n",
        "            C1 = self._C[..., 0] + 1j * self._C[..., 1]\n",
        "            C2 = self._C2[..., 0] + 1j * self._C2[..., 1]\n",
        "            self._output_matrix = jnp.concatenate((C1, C2), axis=-1)\n",
        "        else:\n",
        "            self._output_matrix = self._C[..., 0] + 1j * self._C[..., 1]\n",
        "\n",
        "        self._D = hk.get_parameter(\n",
        "            'D',\n",
        "            [d_model,],\n",
        "            init=hk.initializers.RandomNormal(stddev=1.0)\n",
        "        )\n",
        "\n",
        "        self._delta_t = hk.get_parameter(\n",
        "            'delta_T',\n",
        "            [_state_size, 1],\n",
        "            init=init_log_steps\n",
        "        )\n",
        "        timescale = step_rescale * jnp.exp(self._delta_t[:, 0])\n",
        "\n",
        "        # We could also use the bilinear discretization method, but we'll just stick to zoh for now.\n",
        "        self._state_matrix, self._input_matrix = discretize(self._A, B, timescale)\n",
        "\n",
        "\n",
        "    def __call__(self, inputs):\n",
        "        # Note that this is the exact same function as presented above just with alternate procedures\n",
        "        # depending on the bidirectional and conjugate symmetry arguments\n",
        "\n",
        "        A_elements = self._state_matrix * jnp.ones((inputs.shape[0], self._state_matrix.shape[0]))\n",
        "        Bu_elements = jax.vmap(lambda u: self._input_matrix @ u)(inputs)\n",
        "\n",
        "        _, xs = jax.lax.associative_scan(binary_operator, (A_elements, Bu_elements))\n",
        "\n",
        "        if self.bidirectional:\n",
        "            _, xs2 = jax.lax.associative_scan(binary_operator,\n",
        "                                          (A_elements, Bu_elements),\n",
        "                                          reverse=True)\n",
        "            xs = jnp.concatenate((xs, xs2), axis=-1)\n",
        "\n",
        "        if self.conj_sym:\n",
        "            ys = jax.vmap(lambda x: 2*(self._output_matrix @ x).real)(xs)\n",
        "        else:\n",
        "            ys = jax.vmap(lambda x: (self._output_matrix @ x).real)(xs)\n",
        "\n",
        "        Du = jax.vmap(lambda u: self._D * u)(inputs)\n",
        "\n",
        "        return ys + Du"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n",
          "is_executing": true
        },
        "id": "pu0NZItog6tD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There we have it, a complete S5 layer! Now let's form a block around it using a structure very similar to a transformer block with a Gated Linear Unit (GLU)."
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "BwSd9kcOg6tD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "outputs": [],
      "source": [
        "import dataclasses\n",
        "\n",
        "@dataclasses.dataclass\n",
        "class S5Block(hk.Module):\n",
        "    ssm: S5\n",
        "    d_model: int\n",
        "    dropout_rate: float\n",
        "    prenorm: bool\n",
        "    istraining: bool = True\n",
        "    name: Optional[str] = None\n",
        "\n",
        "    def __post_init__(self):\n",
        "        super(S5Block, self).__post_init__()\n",
        "        # We could use either layer norm or batch norm.\n",
        "        self._norm = hk.LayerNorm(axis=-1, create_scale=True, create_offset=True)\n",
        "        self._linear = hk.Linear(self.d_model)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        skip = x\n",
        "        if self.prenorm:\n",
        "            x = self._norm(x)\n",
        "\n",
        "        x = self.ssm(x)\n",
        "        # There are a couple of other GLU patterns we could use here, but once again I have chosen\n",
        "        # one semi-arbitrarily to avoid cluttering our module with if statements.\n",
        "        x1 = hk.dropout(hk.next_rng_key(), self.dropout_rate, jax.nn.gelu(x))\n",
        "        x = x * jax.nn.sigmoid(self._linear(x1))\n",
        "        x = hk.dropout(hk.next_rng_key(), self.dropout_rate, x)\n",
        "\n",
        "        x = skip + x\n",
        "        if not self.prenorm:\n",
        "            x = self._norm(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n",
          "is_executing": true
        },
        "id": "p9oMO9Tfg6tD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's make a stack of these blocks:"
      ],
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "nxrqTJJXg6tE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "outputs": [],
      "source": [
        "@dataclasses.dataclass\n",
        "class S5Stack(hk.Module):\n",
        "    ssm: S5\n",
        "    d_model: int\n",
        "    n_layers: int\n",
        "    dropout_rate: float\n",
        "    prenorm: bool\n",
        "    istraining: bool = True\n",
        "    name: Optional[str] = None\n",
        "\n",
        "    def __post_init__(self):\n",
        "        super(S5Stack, self).__post_init__(name=self.name)\n",
        "        self._encoder = hk.Linear(self.d_model)\n",
        "        self._layers = [\n",
        "            S5Block(\n",
        "                ssm=self.ssm,\n",
        "                d_model=self.d_model,\n",
        "                dropout_rate=self.dropout_rate,\n",
        "                istraining=self.istraining,\n",
        "                prenorm=self.prenorm,\n",
        "            )\n",
        "            for _ in range(self.n_layers)\n",
        "        ]\n",
        "\n",
        "    def __call__(self, x):\n",
        "        x = self._encoder(x)\n",
        "        for layer in self._layers:\n",
        "            x = layer(x)\n",
        "        return x"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n",
          "is_executing": true
        },
        "id": "Sa0yRsNsg6tE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementing Tokenised prediction in S5"
      ],
      "metadata": {
        "id": "uAD9a1Oyr2Hr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class StockPriceTokeniser:\n",
        "    \"\"\"\n",
        "    A simple quantisation tokeniser.\n",
        "    Converts continuous stock prices into discrete integer tokens.\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size: int):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.bins = None\n",
        "        self.min_price = 0.0\n",
        "        self.max_price = 1.0\n",
        "\n",
        "    def fit(self, data: jnp.ndarray):\n",
        "        \"\"\"Fits the tokeniser to the data to find min/max prices.\"\"\"\n",
        "        self.min_price = data.min()\n",
        "        self.max_price = data.max()\n",
        "        # Create vocab_size-1 thresholds for binning\n",
        "        self.bins = jnp.linspace(self.min_price, self.max_price, self.vocab_size - 1)\n",
        "        print(f\"[*] Tokeniser fitted: min={self.min_price:.2f}, max={self.max_price:.2f}\")\n",
        "\n",
        "    def encode(self, prices: jnp.ndarray) -> jnp.ndarray:\n",
        "        \"\"\"Converts a sequence of prices into token IDs.\"\"\"\n",
        "        if self.bins is None:\n",
        "            raise ValueError(\"Tokeniser must be fitted before encoding.\")\n",
        "        # jnp.digitize finds which bin each price belongs to\n",
        "        # The result is an integer ID from 0 to vocab_size-1\n",
        "        token_ids = jnp.digitize(prices, self.bins)\n",
        "        return token_ids.astype(jnp.int32)\n",
        "\n",
        "    def decode(self, token_ids: jnp.ndarray) -> jnp.ndarray:\n",
        "        \"\"\"Converts a sequence of token IDs back into approximate prices (bin centres).\"\"\"\n",
        "        if self.bins is None:\n",
        "            raise ValueError(\"Tokeniser must be fitted before decoding.\")\n",
        "\n",
        "        # Create bin centres\n",
        "        bin_edges = jnp.concatenate([\n",
        "            jnp.array([self.min_price]),\n",
        "            self.bins,\n",
        "            jnp.array([self.max_price])\n",
        "        ])\n",
        "        bin_centres = (bin_edges[:-1] + bin_edges[1:]) / 2.0\n",
        "\n",
        "        # Map IDs to bin centres\n",
        "        prices = bin_centres[token_ids]\n",
        "        return prices"
      ],
      "metadata": {
        "id": "agNh6ujhBKZ6"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "@dataclasses.dataclass\n",
        "class S5Forecaster(hk.Module):\n",
        "    ssm: S5\n",
        "    d_model: int\n",
        "    n_layers: int\n",
        "    vocab_size: int  # New: Number of tokens in our price vocabulary\n",
        "    dropout_rate: float\n",
        "    prenorm: bool = True\n",
        "    istraining: bool = True\n",
        "    name: Optional[str] = None\n",
        "\n",
        "    def __post_init__(self):\n",
        "        super(S5Forecaster, self).__post_init__(name=self.name)\n",
        "\n",
        "        # 1. Token Embedding Layer\n",
        "        self._embedding = hk.Embed(\n",
        "            vocab_size=self.vocab_size,\n",
        "            embed_dim=self.d_model\n",
        "        )\n",
        "\n",
        "        # 2. S5 Stack\n",
        "        self._s5_stack = S5Stack(\n",
        "            ssm=self.ssm,\n",
        "            d_model=self.d_model,\n",
        "            n_layers=self.n_layers,\n",
        "            dropout_rate=self.dropout_rate,\n",
        "            istraining=self.istraining,\n",
        "            prenorm=self.prenorm,\n",
        "        )\n",
        "\n",
        "        # 3. Decoder Head (projects back to vocabulary)\n",
        "        self._decoder = hk.Linear(self.vocab_size)\n",
        "\n",
        "    def __call__(self, token_ids: jnp.ndarray) -> jnp.ndarray:\n",
        "        \"\"\"\n",
        "        Input: sequence of token IDs, shape [SeqLen]\n",
        "        Output: sequence of logits, shape [SeqLen, VocabSize]\n",
        "        \"\"\"\n",
        "        # 1. Embed tokens\n",
        "        x = self._embedding(token_ids)  # [SeqLen] -> [SeqLen, d_model]\n",
        "\n",
        "        # 2. Process with S5\n",
        "        x = self._s5_stack(x)           # [SeqLen, d_model] -> [SeqLen, d_model]\n",
        "\n",
        "        # 3. Decode to logits\n",
        "        logits = self._decoder(x)       # [SeqLen, d_model] -> [SeqLen, VocabSize]\n",
        "\n",
        "        return logits"
      ],
      "metadata": {
        "id": "AxLXqWNsBOn8"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "def generate_synthetic_stock_data(num_series, seq_length):\n",
        "    \"\"\"Generates a batch of noisy sine waves as dummy stock data.\"\"\"\n",
        "    t = jnp.linspace(0, 4 * jnp.pi, seq_length)\n",
        "    # Generate a few different frequencies\n",
        "    base_freqs = jnp.array([1.0, 2.0, 4.0])\n",
        "    series = []\n",
        "    key = jax.random.PRNGKey(42)\n",
        "\n",
        "    for i in range(num_series):\n",
        "        key, subkey1, subkey2, subkey3, subkey4 = jax.random.split(key, 5)\n",
        "\n",
        "        # Random phase, amplitude, and noise\n",
        "        phase = jax.random.uniform(subkey1) * 2 * jnp.pi\n",
        "        amplitude = jax.random.uniform(subkey2) * 0.5 + 0.5\n",
        "        noise = jax.random.normal(subkey3, shape=(seq_length,)) * 0.1\n",
        "\n",
        "        # Pick a base frequency\n",
        "        freq = jax.random.choice(subkey4, base_freqs)\n",
        "\n",
        "        # Combine into a series and add a trend\n",
        "        trend = jnp.linspace(0, 1.0, seq_length) * (jax.random.uniform(key)-0.5) * 2\n",
        "        s = 100 + amplitude * jnp.sin(freq * t + phase) + noise + trend\n",
        "        series.append(s)\n",
        "\n",
        "    return jnp.stack(series)\n",
        "\n",
        "def create_dataset(\n",
        "    price_data: jnp.ndarray,\n",
        "    tokeniser: StockPriceTokeniser,\n",
        "    seq_len: int\n",
        "):\n",
        "    \"\"\"\n",
        "    Encodes price data and creates autoregressive (x, y) pairs.\n",
        "    x = [t_0, t_1, ..., t_n-1]\n",
        "    y = [t_1, t_2, ..., t_n]\n",
        "    \"\"\"\n",
        "    # 1. Tokenise the entire dataset\n",
        "    token_data = tokeniser.encode(price_data) # [NumSeries, TotalLength]\n",
        "\n",
        "    # 2. Create overlapping sequences\n",
        "    # We'll use a simple approach: just take the first seq_len+1 tokens\n",
        "    # A real implementation would use sliding windows.\n",
        "\n",
        "    input_seq_len = seq_len\n",
        "    target_seq_len = seq_len\n",
        "\n",
        "    # Ensure data is long enough\n",
        "    assert token_data.shape[1] > seq_len, \"Data is shorter than sequence length\"\n",
        "\n",
        "    # Input tokens: [t_0, ..., t_seq_len-1]\n",
        "    inputs = token_data[:, :input_seq_len]\n",
        "\n",
        "    # Target tokens (shifted by 1): [t_1, ..., t_seq_len]\n",
        "    targets = token_data[:, 1:input_seq_len+1]\n",
        "\n",
        "    return inputs, targets"
      ],
      "metadata": {
        "id": "F3ibHTGXBRpg"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "# --- Hyperparameters ---\n",
        "STATE_SIZE: int = 128\n",
        "D_MODEL: int = 64\n",
        "N_LAYERS: int = 3\n",
        "N_BLOCKS: int = 4\n",
        "DROPOUT_RATE: float = 0.1\n",
        "LEARNING_RATE: float = 1e-3\n",
        "SEED = 0\n",
        "\n",
        "# New Hyperparameters\n",
        "VOCAB_SIZE: int = 1024       # Number of \"price bins\"\n",
        "SEQ_LEN: int = 784           # Sequence length (kept from MNIST for comparison)\n",
        "TOTAL_DATA_LEN: int = SEQ_LEN + 50 # Length of underlying data\n",
        "BATCH_SIZE: int = 64\n",
        "EPOCHS: int = 50\n",
        "\n",
        "# %%\n",
        "# --- Optimizer and Training State ---\n",
        "class TrainingState(NamedTuple):\n",
        "    params: hk.Params\n",
        "    opt_state: optax.OptState\n",
        "    rng_key: jnp.ndarray\n",
        "\n",
        "optim = optax.adam(LEARNING_RATE)\n",
        "\n",
        "# %%\n",
        "# --- Loss Function ---\n",
        "def loss_fn(\n",
        "    params: hk.Params,\n",
        "    rng_key: jnp.ndarray,\n",
        "    model: hk.Transformed,\n",
        "    inputs: jnp.ndarray,  # [Batch, SeqLen]\n",
        "    targets: jnp.ndarray  # [Batch, SeqLen]\n",
        ") -> jnp.ndarray:\n",
        "\n",
        "    # Forward pass: Get logits [Batch, SeqLen, VocabSize]\n",
        "    logits = model.apply(params, rng_key, inputs)\n",
        "\n",
        "    # Calculate cross-entropy loss for sequences\n",
        "    # This compares logits[t] with targets[t] for all t\n",
        "    loss_per_token = optax.softmax_cross_entropy_with_integer_labels(\n",
        "        logits=logits,\n",
        "        labels=targets\n",
        "    )\n",
        "\n",
        "    # Return the mean loss over the batch and sequence\n",
        "    return jnp.mean(loss_per_token)\n",
        "\n",
        "# --- Accuracy Function ---\n",
        "def accuracy_fn(\n",
        "    params: hk.Params,\n",
        "    rng_key: jnp.ndarray,\n",
        "    model: hk.Transformed,\n",
        "    inputs: jnp.ndarray,\n",
        "    targets: jnp.ndarray\n",
        ") -> jnp.ndarray:\n",
        "\n",
        "    logits = model.apply(params, rng_key, inputs)\n",
        "\n",
        "    # Get the token ID with the highest probability\n",
        "    predictions = jnp.argmax(logits, axis=-1)\n",
        "\n",
        "    # Compare predictions to the actual next token\n",
        "    accuracy = jnp.mean(predictions == targets)\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "LAnUOIrZBOxg"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "# --- Update and Evaluate Functions ---\n",
        "_Metrics = MutableMapping[str, Any]\n",
        "\n",
        "@partial(jax.jit, static_argnums=(2,))\n",
        "def update(\n",
        "    state: TrainingState,\n",
        "    batch: Tuple[jnp.ndarray, jnp.ndarray],\n",
        "    model: hk.Transformed,\n",
        ") -> Tuple[TrainingState, _Metrics]:\n",
        "\n",
        "    inputs, targets = batch\n",
        "    rng_key, next_rng_key = jax.random.split(state.rng_key)\n",
        "\n",
        "    # Calculate loss and gradients\n",
        "    (loss, gradients) = jax.value_and_grad(loss_fn)(\n",
        "        state.params,\n",
        "        rng_key,\n",
        "        model,\n",
        "        inputs,\n",
        "        targets\n",
        "    )\n",
        "\n",
        "    updates, new_opt_state = optim.update(gradients, state.opt_state, state.params)\n",
        "    new_params = optax.apply_updates(state.params, updates)\n",
        "\n",
        "    new_state = TrainingState(\n",
        "        params=new_params,\n",
        "        opt_state=new_opt_state,\n",
        "        rng_key=next_rng_key,\n",
        "    )\n",
        "\n",
        "    metrics = {'loss': loss}\n",
        "    return new_state, metrics\n",
        "\n",
        "\n",
        "@partial(jax.jit, static_argnums=(2,))\n",
        "def evaluate(\n",
        "    state: TrainingState,\n",
        "    batch: Tuple[jnp.ndarray, jnp.ndarray],\n",
        "    model: hk.Transformed,\n",
        ") -> _Metrics:\n",
        "\n",
        "    inputs, targets = batch\n",
        "\n",
        "    # We can use the same state.rng_key for eval; no dropout updates\n",
        "    loss = loss_fn(state.params, state.rng_key, model, inputs, targets)\n",
        "    accuracy = accuracy_fn(state.params, state.rng_key, model, inputs, targets)\n",
        "\n",
        "    metrics = {\n",
        "        'loss': loss,\n",
        "        'accuracy': accuracy\n",
        "    }\n",
        "    return metrics"
      ],
      "metadata": {
        "id": "BCVmwARsBVkc"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "# --- Set random seeds ---\n",
        "torch.random.manual_seed(SEED)\n",
        "key = jax.random.PRNGKey(SEED)\n",
        "rng, init_rng = jax.random.split(key)\n",
        "\n",
        "# %%\n",
        "# --- Create Dataset ---\n",
        "print(\"[*] Generating synthetic stock data...\")\n",
        "raw_price_data = generate_synthetic_stock_data(\n",
        "    num_series=BATCH_SIZE*10, # 10 batches for training\n",
        "    seq_length=TOTAL_DATA_LEN\n",
        ")\n",
        "\n",
        "print(\"[*] Fitting tokeniser...\")\n",
        "tokeniser = StockPriceTokeniser(vocab_size=VOCAB_SIZE)\n",
        "tokeniser.fit(raw_price_data)\n",
        "\n",
        "print(\"[*] Creating tokenised (x, y) pairs...\")\n",
        "# For this example, we'll just use one large batch\n",
        "# A real pipeline would use a proper data loader and batching\n",
        "\n",
        "# For validation, we generate new data\n",
        "val_price_data = generate_synthetic_stock_data(BATCH_SIZE, TOTAL_DATA_LEN)\n",
        "val_inputs, val_targets = create_dataset(val_price_data, tokeniser, SEQ_LEN)\n",
        "val_batch = (val_inputs, val_targets)\n",
        "\n",
        "# For training data, we'll create multiple batches\n",
        "train_price_data = generate_synthetic_stock_data(BATCH_SIZE * 10, TOTAL_DATA_LEN)\n",
        "train_inputs, train_targets = create_dataset(train_price_data, tokeniser, SEQ_LEN)\n",
        "\n",
        "num_batches = train_inputs.shape[0] // BATCH_SIZE\n",
        "train_batches = []\n",
        "for i in range(num_batches):\n",
        "    start = i * BATCH_SIZE\n",
        "    end = (i + 1) * BATCH_SIZE\n",
        "    train_batches.append((train_inputs[start:end], train_targets[start:end]))\n",
        "\n",
        "\n",
        "# %%\n",
        "# --- Initialise Model ---\n",
        "\n",
        "# Use a dummy batch for initialisation\n",
        "dummy_batch = train_batches[0][0]\n",
        "\n",
        "@hk.transform\n",
        "def forward(x) -> hk.transform:\n",
        "    # Build the S5 component\n",
        "    s5_ssm = S5(\n",
        "        state_size=STATE_SIZE,\n",
        "        d_model=D_MODEL,\n",
        "        n_blocks=N_BLOCKS,\n",
        "    )\n",
        "\n",
        "    # Build the full forecaster model\n",
        "    neural_net = S5Forecaster(\n",
        "        ssm=s5_ssm,\n",
        "        d_model=D_MODEL,\n",
        "        n_layers=N_LAYERS,\n",
        "        vocab_size=VOCAB_SIZE,\n",
        "        dropout_rate=DROPOUT_RATE,\n",
        "        istraining=True # We can toggle this for eval if needed\n",
        "    )\n",
        "\n",
        "    # vmap over the batch dimension\n",
        "    return hk.vmap(neural_net, in_axes=0, split_rng=False)(x)\n",
        "\n",
        "# %%\n",
        "# Set state\n",
        "initial_params = forward.init(init_rng, dummy_batch)\n",
        "initial_opt_state = optim.init(initial_params)\n",
        "\n",
        "state = TrainingState(\n",
        "    params=initial_params,\n",
        "    opt_state=initial_opt_state,\n",
        "    rng_key=rng\n",
        ")"
      ],
      "metadata": {
        "id": "JRjz8hngBX20",
        "outputId": "e0c21ac2-a148-4cfb-8ef4-6dd5cd3fab78",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[*] Generating synthetic stock data...\n",
            "[*] Fitting tokeniser...\n",
            "[*] Tokeniser fitted: min=97.93, max=102.02\n",
            "[*] Creating tokenised (x, y) pairs...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "# --- Training Loop ---\n",
        "print(\"\\n[*] Starting training...\")\n",
        "for epoch in range(EPOCHS):\n",
        "\n",
        "    # --- Training ---\n",
        "    epoch_losses = []\n",
        "    for batch in tqdm(train_batches, desc=f\"Epoch {epoch + 1}/{EPOCHS} [Train]\"):\n",
        "        state, metrics = update(state, batch, forward)\n",
        "        epoch_losses.append(metrics['loss'])\n",
        "\n",
        "    train_loss = jnp.mean(jnp.array(epoch_losses))\n",
        "\n",
        "    # --- Validation ---\n",
        "    val_metrics = evaluate(state, val_batch, forward)\n",
        "\n",
        "    print(\n",
        "        f\"\\n=>> Epoch {epoch + 1} Metrics ===\\n\"\n",
        "        f\"\\tTrain Loss: {train_loss:.5f}\\n\"\n",
        "        f\"\\t Val. Loss: {val_metrics['loss']:.5f} -- Val. Accuracy: {val_metrics['accuracy']:.4f}\"\n",
        "    )\n",
        "\n",
        "print(\"[*] Training complete.\")"
      ],
      "metadata": {
        "id": "Aaljovq0BZtg",
        "outputId": "c81da2b9-faa3-44e6-8987-7fa3d3a7db4a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[*] Starting training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 1/50 [Train]:   0%|          | 0/10 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/jax/_src/lax/lax.py:5473: ComplexWarning: Casting complex values to real discards the imaginary part\n",
            "  x_bar = _convert_element_type(x_bar, x.aval.dtype, x.aval.weak_type)\n",
            "Epoch 1/50 [Train]: 100%|██████████| 10/10 [00:24<00:00,  2.48s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=>> Epoch 1 Metrics ===\n",
            "\tTrain Loss: 12.46808\n",
            "\t Val. Loss: 10.19843 -- Val. Accuracy: 0.0009\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/50 [Train]: 100%|██████████| 10/10 [00:00<00:00, 565.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=>> Epoch 2 Metrics ===\n",
            "\tTrain Loss: 9.30281\n",
            "\t Val. Loss: 8.49791 -- Val. Accuracy: 0.0010\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/50 [Train]: 100%|██████████| 10/10 [00:00<00:00, 586.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=>> Epoch 3 Metrics ===\n",
            "\tTrain Loss: 8.11612\n",
            "\t Val. Loss: 7.74346 -- Val. Accuracy: 0.0014\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/50 [Train]: 100%|██████████| 10/10 [00:00<00:00, 647.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=>> Epoch 4 Metrics ===\n",
            "\tTrain Loss: 7.54373\n",
            "\t Val. Loss: 7.33622 -- Val. Accuracy: 0.0017\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/50 [Train]: 100%|██████████| 10/10 [00:00<00:00, 490.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=>> Epoch 5 Metrics ===\n",
            "\tTrain Loss: 7.21346\n",
            "\t Val. Loss: 7.08273 -- Val. Accuracy: 0.0021\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/50 [Train]: 100%|██████████| 10/10 [00:00<00:00, 561.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=>> Epoch 6 Metrics ===\n",
            "\tTrain Loss: 7.00340\n",
            "\t Val. Loss: 6.91889 -- Val. Accuracy: 0.0021\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/50 [Train]: 100%|██████████| 10/10 [00:00<00:00, 610.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=>> Epoch 7 Metrics ===\n",
            "\tTrain Loss: 6.86186\n",
            "\t Val. Loss: 6.80637 -- Val. Accuracy: 0.0024\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/50 [Train]: 100%|██████████| 10/10 [00:00<00:00, 444.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=>> Epoch 8 Metrics ===\n",
            "\tTrain Loss: 6.75810\n",
            "\t Val. Loss: 6.71610 -- Val. Accuracy: 0.0025\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/50 [Train]: 100%|██████████| 10/10 [00:00<00:00, 655.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=>> Epoch 9 Metrics ===\n",
            "\tTrain Loss: 6.67960\n",
            "\t Val. Loss: 6.64613 -- Val. Accuracy: 0.0027\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/50 [Train]: 100%|██████████| 10/10 [00:00<00:00, 547.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=>> Epoch 10 Metrics ===\n",
            "\tTrain Loss: 6.61104\n",
            "\t Val. Loss: 6.57914 -- Val. Accuracy: 0.0029\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11/50 [Train]: 100%|██████████| 10/10 [00:00<00:00, 624.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=>> Epoch 11 Metrics ===\n",
            "\tTrain Loss: 6.54758\n",
            "\t Val. Loss: 6.51559 -- Val. Accuracy: 0.0029\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12/50 [Train]: 100%|██████████| 10/10 [00:00<00:00, 590.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=>> Epoch 12 Metrics ===\n",
            "\tTrain Loss: 6.47967\n",
            "\t Val. Loss: 6.44455 -- Val. Accuracy: 0.0038\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13/50 [Train]: 100%|██████████| 10/10 [00:00<00:00, 449.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=>> Epoch 13 Metrics ===\n",
            "\tTrain Loss: 6.39771\n",
            "\t Val. Loss: 6.34773 -- Val. Accuracy: 0.0046\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14/50 [Train]: 100%|██████████| 10/10 [00:00<00:00, 430.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=>> Epoch 14 Metrics ===\n",
            "\tTrain Loss: 6.29170\n",
            "\t Val. Loss: 6.23048 -- Val. Accuracy: 0.0048\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15/50 [Train]: 100%|██████████| 10/10 [00:00<00:00, 422.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=>> Epoch 15 Metrics ===\n",
            "\tTrain Loss: 6.16541\n",
            "\t Val. Loss: 6.09156 -- Val. Accuracy: 0.0056\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 16/50 [Train]: 100%|██████████| 10/10 [00:00<00:00, 298.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=>> Epoch 16 Metrics ===\n",
            "\tTrain Loss: 6.02273\n",
            "\t Val. Loss: 5.94997 -- Val. Accuracy: 0.0059\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 17/50 [Train]: 100%|██████████| 10/10 [00:00<00:00, 601.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=>> Epoch 17 Metrics ===\n",
            "\tTrain Loss: 5.87932\n",
            "\t Val. Loss: 5.79894 -- Val. Accuracy: 0.0072\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 18/50 [Train]: 100%|██████████| 10/10 [00:00<00:00, 573.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=>> Epoch 18 Metrics ===\n",
            "\tTrain Loss: 5.73446\n",
            "\t Val. Loss: 5.65598 -- Val. Accuracy: 0.0078\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 19/50 [Train]: 100%|██████████| 10/10 [00:00<00:00, 620.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=>> Epoch 19 Metrics ===\n",
            "\tTrain Loss: 5.60712\n",
            "\t Val. Loss: 5.53742 -- Val. Accuracy: 0.0082\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 20/50 [Train]: 100%|██████████| 10/10 [00:00<00:00, 662.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=>> Epoch 20 Metrics ===\n",
            "\tTrain Loss: 5.49701\n",
            "\t Val. Loss: 5.44088 -- Val. Accuracy: 0.0085\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 21/50 [Train]: 100%|██████████| 10/10 [00:00<00:00, 590.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=>> Epoch 21 Metrics ===\n",
            "\tTrain Loss: 5.40769\n",
            "\t Val. Loss: 5.36038 -- Val. Accuracy: 0.0087\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 22/50 [Train]: 100%|██████████| 10/10 [00:00<00:00, 614.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=>> Epoch 22 Metrics ===\n",
            "\tTrain Loss: 5.33156\n",
            "\t Val. Loss: 5.28914 -- Val. Accuracy: 0.0096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 23/50 [Train]: 100%|██████████| 10/10 [00:00<00:00, 577.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=>> Epoch 23 Metrics ===\n",
            "\tTrain Loss: 5.26743\n",
            "\t Val. Loss: 5.23122 -- Val. Accuracy: 0.0099\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 24/50 [Train]: 100%|██████████| 10/10 [00:00<00:00, 400.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=>> Epoch 24 Metrics ===\n",
            "\tTrain Loss: 5.21280\n",
            "\t Val. Loss: 5.18003 -- Val. Accuracy: 0.0108\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 25/50 [Train]: 100%|██████████| 10/10 [00:00<00:00, 578.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=>> Epoch 25 Metrics ===\n",
            "\tTrain Loss: 5.16546\n",
            "\t Val. Loss: 5.13222 -- Val. Accuracy: 0.0122\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 26/50 [Train]: 100%|██████████| 10/10 [00:00<00:00, 654.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=>> Epoch 26 Metrics ===\n",
            "\tTrain Loss: 5.12275\n",
            "\t Val. Loss: 5.09496 -- Val. Accuracy: 0.0111\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 27/50 [Train]: 100%|██████████| 10/10 [00:00<00:00, 590.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=>> Epoch 27 Metrics ===\n",
            "\tTrain Loss: 5.08584\n",
            "\t Val. Loss: 5.06363 -- Val. Accuracy: 0.0128\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 28/50 [Train]: 100%|██████████| 10/10 [00:00<00:00, 633.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=>> Epoch 28 Metrics ===\n",
            "\tTrain Loss: 5.05469\n",
            "\t Val. Loss: 5.03290 -- Val. Accuracy: 0.0119\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 29/50 [Train]: 100%|██████████| 10/10 [00:00<00:00, 638.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=>> Epoch 29 Metrics ===\n",
            "\tTrain Loss: 5.02434\n",
            "\t Val. Loss: 5.00242 -- Val. Accuracy: 0.0129\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 30/50 [Train]: 100%|██████████| 10/10 [00:00<00:00, 612.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=>> Epoch 30 Metrics ===\n",
            "\tTrain Loss: 4.99829\n",
            "\t Val. Loss: 4.98053 -- Val. Accuracy: 0.0130\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 31/50 [Train]: 100%|██████████| 10/10 [00:00<00:00, 628.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=>> Epoch 31 Metrics ===\n",
            "\tTrain Loss: 4.97569\n",
            "\t Val. Loss: 4.96022 -- Val. Accuracy: 0.0137\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 32/50 [Train]: 100%|██████████| 10/10 [00:00<00:00, 593.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=>> Epoch 32 Metrics ===\n",
            "\tTrain Loss: 4.95492\n",
            "\t Val. Loss: 4.94165 -- Val. Accuracy: 0.0138\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 33/50 [Train]: 100%|██████████| 10/10 [00:00<00:00, 639.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=>> Epoch 33 Metrics ===\n",
            "\tTrain Loss: 4.93465\n",
            "\t Val. Loss: 4.92558 -- Val. Accuracy: 0.0129\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 34/50 [Train]: 100%|██████████| 10/10 [00:00<00:00, 568.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=>> Epoch 34 Metrics ===\n",
            "\tTrain Loss: 4.91839\n",
            "\t Val. Loss: 4.90571 -- Val. Accuracy: 0.0139\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 35/50 [Train]: 100%|██████████| 10/10 [00:00<00:00, 609.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=>> Epoch 35 Metrics ===\n",
            "\tTrain Loss: 4.90259\n",
            "\t Val. Loss: 4.88869 -- Val. Accuracy: 0.0141\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 36/50 [Train]: 100%|██████████| 10/10 [00:00<00:00, 632.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=>> Epoch 36 Metrics ===\n",
            "\tTrain Loss: 4.88869\n",
            "\t Val. Loss: 4.87734 -- Val. Accuracy: 0.0145\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 37/50 [Train]: 100%|██████████| 10/10 [00:00<00:00, 475.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=>> Epoch 37 Metrics ===\n",
            "\tTrain Loss: 4.87473\n",
            "\t Val. Loss: 4.86327 -- Val. Accuracy: 0.0140\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 38/50 [Train]: 100%|██████████| 10/10 [00:00<00:00, 689.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=>> Epoch 38 Metrics ===\n",
            "\tTrain Loss: 4.86278\n",
            "\t Val. Loss: 4.85276 -- Val. Accuracy: 0.0149\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 39/50 [Train]: 100%|██████████| 10/10 [00:00<00:00, 633.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=>> Epoch 39 Metrics ===\n",
            "\tTrain Loss: 4.85199\n",
            "\t Val. Loss: 4.84259 -- Val. Accuracy: 0.0153\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 40/50 [Train]: 100%|██████████| 10/10 [00:00<00:00, 651.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=>> Epoch 40 Metrics ===\n",
            "\tTrain Loss: 4.84168\n",
            "\t Val. Loss: 4.83211 -- Val. Accuracy: 0.0144\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 41/50 [Train]: 100%|██████████| 10/10 [00:00<00:00, 590.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=>> Epoch 41 Metrics ===\n",
            "\tTrain Loss: 4.83290\n",
            "\t Val. Loss: 4.82312 -- Val. Accuracy: 0.0161\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 42/50 [Train]: 100%|██████████| 10/10 [00:00<00:00, 511.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=>> Epoch 42 Metrics ===\n",
            "\tTrain Loss: 4.82426\n",
            "\t Val. Loss: 4.81643 -- Val. Accuracy: 0.0157\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 43/50 [Train]: 100%|██████████| 10/10 [00:00<00:00, 372.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=>> Epoch 43 Metrics ===\n",
            "\tTrain Loss: 4.81727\n",
            "\t Val. Loss: 4.80711 -- Val. Accuracy: 0.0168\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 44/50 [Train]: 100%|██████████| 10/10 [00:00<00:00, 415.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=>> Epoch 44 Metrics ===\n",
            "\tTrain Loss: 4.80905\n",
            "\t Val. Loss: 4.80202 -- Val. Accuracy: 0.0155\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 45/50 [Train]: 100%|██████████| 10/10 [00:00<00:00, 393.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=>> Epoch 45 Metrics ===\n",
            "\tTrain Loss: 4.80297\n",
            "\t Val. Loss: 4.79927 -- Val. Accuracy: 0.0158\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 46/50 [Train]: 100%|██████████| 10/10 [00:00<00:00, 547.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=>> Epoch 46 Metrics ===\n",
            "\tTrain Loss: 4.79553\n",
            "\t Val. Loss: 4.78919 -- Val. Accuracy: 0.0163\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 47/50 [Train]: 100%|██████████| 10/10 [00:00<00:00, 630.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=>> Epoch 47 Metrics ===\n",
            "\tTrain Loss: 4.78990\n",
            "\t Val. Loss: 4.78319 -- Val. Accuracy: 0.0170\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 48/50 [Train]: 100%|██████████| 10/10 [00:00<00:00, 606.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=>> Epoch 48 Metrics ===\n",
            "\tTrain Loss: 4.78469\n",
            "\t Val. Loss: 4.78068 -- Val. Accuracy: 0.0170\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 49/50 [Train]: 100%|██████████| 10/10 [00:00<00:00, 607.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=>> Epoch 49 Metrics ===\n",
            "\tTrain Loss: 4.77954\n",
            "\t Val. Loss: 4.77314 -- Val. Accuracy: 0.0162\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 50/50 [Train]: 100%|██████████| 10/10 [00:00<00:00, 568.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=>> Epoch 50 Metrics ===\n",
            "\tTrain Loss: 4.77450\n",
            "\t Val. Loss: 4.76984 -- Val. Accuracy: 0.0173\n",
            "[*] Training complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %%\n",
        "@partial(jax.jit, static_argnums=(0, 2))\n",
        "def generate_forecast(\n",
        "    model: hk.Transformed,\n",
        "    params: hk.Params,\n",
        "    steps_to_forecast: int,\n",
        "    prompt_tokens: jnp.ndarray, # Shape [SeqLen]\n",
        "    rng_key: jnp.ndarray\n",
        "):\n",
        "    \"\"\"\n",
        "    Autoregressively generates token predictions.\n",
        "    \"\"\"\n",
        "\n",
        "    @hk.transform\n",
        "    def forward_generate(x) -> hk.transform:\n",
        "        # Build the S5 component\n",
        "        s5_ssm = S5(\n",
        "            state_size=STATE_SIZE,\n",
        "            d_model=D_MODEL,\n",
        "            n_blocks=N_BLOCKS,\n",
        "        )\n",
        "        # Build the full forecaster model, ensuring istraining=False\n",
        "        neural_net = S5Forecaster(\n",
        "            ssm=s5_ssm,\n",
        "            d_model=D_MODEL,\n",
        "            n_layers=N_LAYERS,\n",
        "            vocab_size=VOCAB_SIZE,\n",
        "            dropout_rate=0.0, # No dropout for inference\n",
        "            istraining=False,\n",
        "        )\n",
        "        # NOTE: No vmap, we process a single sequence\n",
        "        return neural_net(x)\n",
        "\n",
        "\n",
        "    def scan_body(carry, _):\n",
        "        \"\"\"\n",
        "        The body of the scan function.\n",
        "        'carry' contains (current_tokens, rng_key)\n",
        "        \"\"\"\n",
        "        current_tokens, rng_key = carry\n",
        "\n",
        "        # Apply the model\n",
        "        # We only need logits for the *very last* token to predict the next one\n",
        "        logits = forward_generate.apply(params, rng_key, current_tokens)\n",
        "        last_token_logits = logits[-1] # Shape [VocabSize]\n",
        "\n",
        "        # Get next token prediction (greedy)\n",
        "        # For better results, you could use jax.random.categorical to sample\n",
        "        next_token = jnp.argmax(last_token_logits, axis=-1)\n",
        "        next_token = next_token.astype(jnp.int32)\n",
        "\n",
        "        # Create the new token sequence\n",
        "        # We roll the window: drop the first token, append the new one\n",
        "        new_tokens = jnp.roll(current_tokens, -1)\n",
        "        new_tokens = new_tokens.at[-1].set(next_token)\n",
        "\n",
        "        # Split RNG for next step (though not strictly needed for argmax)\n",
        "        rng_key, _ = jax.random.split(rng_key)\n",
        "\n",
        "        # Return new state and the token we just predicted\n",
        "        return (new_tokens, rng_key), next_token\n",
        "\n",
        "    # Initial state for the scan\n",
        "    initial_carry = (prompt_tokens, rng_key)\n",
        "\n",
        "    # Run the scan to generate 'steps_to_forecast' new tokens\n",
        "    _, predicted_token_sequence = jax.lax.scan(\n",
        "        scan_body, initial_carry, None, length=steps_to_forecast\n",
        "    )\n",
        "\n",
        "    return predicted_token_sequence\n",
        "\n",
        "# %%\n",
        "# --- Run a Forecast ---\n",
        "\n",
        "# 1. Get a prompt from the validation data\n",
        "# We'll use the first sequence from our validation batch\n",
        "prompt_price_data = val_price_data[0, :SEQ_LEN]\n",
        "prompt_token_data = tokeniser.encode(prompt_price_data)\n",
        "\n",
        "# 2. Define how many steps to forecast\n",
        "FORECAST_HORIZON = 50\n",
        "\n",
        "print(f\"[*] Generating {FORECAST_HORIZON}-step forecast...\")\n",
        "\n",
        "# 3. Generate the sequence of *future token IDs*\n",
        "# We need to use the *training* state's params, but with a new model\n",
        "# transform set to istraining=False.\n",
        "predicted_tokens = generate_forecast(\n",
        "    forward, # Use the hk.transform, not the instance\n",
        "    state.params,\n",
        "    FORECAST_HORIZON,\n",
        "    prompt_token_data,\n",
        "    state.rng_key\n",
        ")\n",
        "\n",
        "# 4. Decode the tokens back into prices\n",
        "predicted_prices = tokeniser.decode(predicted_tokens)\n",
        "\n",
        "# 5. Get the \"ground truth\" prices for comparison\n",
        "ground_truth_prices = val_price_data[0, SEQ_LEN : SEQ_LEN + FORECAST_HORIZON]\n",
        "\n",
        "print(\"[*] Forecast complete.\")\n",
        "print(\"\\n--- Forecast vs. Ground Truth ---\")\n",
        "for i in range(FORECAST_HORIZON):\n",
        "    print(f\"Step {i+1}: Predicted={predicted_prices[i]:.2f}, Actual={ground_truth_prices[i]:.2f}\")"
      ],
      "metadata": {
        "id": "6VYO1NpuBbkZ",
        "outputId": "11f61f3d-5a1d-406d-8f61-178c077d1d54",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[*] Generating 50-step forecast...\n",
            "[*] Forecast complete.\n",
            "\n",
            "--- Forecast vs. Ground Truth ---\n",
            "Step 1: Predicted=100.13, Actual=100.16\n",
            "Step 2: Predicted=100.10, Actual=99.96\n",
            "Step 3: Predicted=100.13, Actual=99.98\n",
            "Step 4: Predicted=100.10, Actual=99.96\n",
            "Step 5: Predicted=100.13, Actual=99.96\n",
            "Step 6: Predicted=100.03, Actual=99.84\n",
            "Step 7: Predicted=100.00, Actual=99.84\n",
            "Step 8: Predicted=99.95, Actual=99.95\n",
            "Step 9: Predicted=99.95, Actual=99.95\n",
            "Step 10: Predicted=99.95, Actual=99.96\n",
            "Step 11: Predicted=99.85, Actual=99.87\n",
            "Step 12: Predicted=99.85, Actual=99.94\n",
            "Step 13: Predicted=99.85, Actual=99.69\n",
            "Step 14: Predicted=99.85, Actual=99.73\n",
            "Step 15: Predicted=99.85, Actual=99.47\n",
            "Step 16: Predicted=99.78, Actual=99.75\n",
            "Step 17: Predicted=99.77, Actual=99.89\n",
            "Step 18: Predicted=99.73, Actual=99.67\n",
            "Step 19: Predicted=99.69, Actual=99.67\n",
            "Step 20: Predicted=99.64, Actual=99.39\n",
            "Step 21: Predicted=99.62, Actual=99.57\n",
            "Step 22: Predicted=99.62, Actual=99.58\n",
            "Step 23: Predicted=99.62, Actual=99.63\n",
            "Step 24: Predicted=99.53, Actual=99.68\n",
            "Step 25: Predicted=99.51, Actual=99.70\n",
            "Step 26: Predicted=99.43, Actual=99.45\n",
            "Step 27: Predicted=99.51, Actual=99.50\n",
            "Step 28: Predicted=99.43, Actual=99.51\n",
            "Step 29: Predicted=99.44, Actual=99.43\n",
            "Step 30: Predicted=99.53, Actual=99.41\n",
            "Step 31: Predicted=99.44, Actual=99.48\n",
            "Step 32: Predicted=99.41, Actual=99.56\n",
            "Step 33: Predicted=99.38, Actual=99.46\n",
            "Step 34: Predicted=99.41, Actual=99.32\n",
            "Step 35: Predicted=99.38, Actual=99.34\n",
            "Step 36: Predicted=99.41, Actual=99.24\n",
            "Step 37: Predicted=99.38, Actual=99.29\n",
            "Step 38: Predicted=99.41, Actual=99.30\n",
            "Step 39: Predicted=99.38, Actual=99.38\n",
            "Step 40: Predicted=99.41, Actual=99.29\n",
            "Step 41: Predicted=99.38, Actual=99.14\n",
            "Step 42: Predicted=99.41, Actual=99.21\n",
            "Step 43: Predicted=99.38, Actual=99.36\n",
            "Step 44: Predicted=99.41, Actual=99.38\n",
            "Step 45: Predicted=99.38, Actual=99.41\n",
            "Step 46: Predicted=99.42, Actual=99.06\n",
            "Step 47: Predicted=99.37, Actual=99.28\n",
            "Step 48: Predicted=99.42, Actual=99.26\n",
            "Step 49: Predicted=99.43, Actual=99.22\n",
            "Step 50: Predicted=99.45, Actual=99.19\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}